{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (585936101.py, line 137)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[80], line 137\u001b[0;36m\u001b[0m\n\u001b[0;31m    validation_cifar = datasets.CIFAR10Labeled(root=args.cifar_root,\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import kornia as K\n",
    "import numpy as np\n",
    "import easydict\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import mixmatch\n",
    "import datasets\n",
    "import transformations as custom_transforms\n",
    "import utils\n",
    "import losses\n",
    "import models\n",
    "import ramps \n",
    "import wide_resnet\n",
    "\n",
    "\n",
    "args = easydict.EasyDict()\n",
    "# CIFAR 10 reference setting \n",
    "args.train_iterations = 100000\n",
    "args.K = 2\n",
    "args.T = 0.5\n",
    "args.alpha = 0.75\n",
    "args.lam_u = 75 \n",
    "args.rampup_length = 60000\n",
    "args.labeled_set_size = 4000\n",
    "args.unlabeled_set_size = 45999 # sum of labeled and unlabeled set sizes must be smaller then total size -1 (49999)\n",
    "args.batch_size = 64\n",
    "args.lr = 0.002 \n",
    "args.ewa_coef = 0.99\n",
    "args.device = utils.get_device(1)\n",
    "args.cifar_root = './CIFAR10'\n",
    "args.cifar_download = True\n",
    "args.use_ema = False\n",
    "\n",
    "\n",
    "args.basename = 'foo'\n",
    "args.call_prefix = '-1'\n",
    "args.res_path = './'\n",
    "args.new_log = True\n",
    "\n",
    "args.log_period = 100\n",
    "args.save_period = 10000\n",
    "args.validation_period = 100\n",
    "\n",
    "logpath = 'logs/log-' + args.basename + args.call_prefix + '.txt'\n",
    "logpath = os.path.join(args.res_path, logpath)\n",
    "\n",
    "model_path = 'models/' + args.basename + args.call_prefix +'-'\n",
    "model_path = os.path.join(args.res_path, model_path)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(logpath) or args.new_log:\n",
    "    print(f\"# Starting at {datetime.now()}\",file=open(logpath,'w'),flush=True)\n",
    "else:\n",
    "    print(f\"# Starting at {datetime.now()}\",file=open(logpath,'a'),flush=True)\n",
    "\n",
    "print(f\"with args:\\n\" + \"\\n\".join([f\"{key} : {value}\" for key,value in args.items()]),file=open(logpath,'a'),flush=True)\n",
    "print(f\"logpath: {logpath}\",file=open(logpath,'a'),flush=True)\n",
    "print(f\"modelpath: {model_path}<name>.pt\",file=open(logpath,'a'),flush=True)\n",
    " \n",
    "# Datasets and dataloaders\n",
    "num_classes = 10 \n",
    "base_dataset = torchvision.datasets.CIFAR10(args.cifar_root,train=True,download=args.cifar_download)\n",
    "\n",
    "# Compute mean and std \n",
    "#TODO: std, mean  = datasets.compute_std_mean(base_dataset)\n",
    "mean = torch.Tensor([0.4914, 0.4822, 0.4465]) \n",
    "std = torch.Tensor([0.2471, 0.2435, 0.2616]) \n",
    "_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=mean,std=std)])\n",
    "\n",
    "t = base_dataset.targets\n",
    "labeled_ratio = args.labeled_set_size / len(base_dataset)\n",
    "unlabeled_ratio = args.unlabeled_set_size / len(base_dataset)\n",
    "\n",
    "indicies_val,t_val,indicies_labeled,t_labeled,indicies_unlabeled,t_unlabeled = datasets.train_test_val_split([range(len(t)),t],\n",
    "                                                                                                split=(labeled_ratio,unlabeled_ratio), \n",
    "                                                                                                shuffle=True,\n",
    "                                                                                                random_state=None,\n",
    "                                                                                                stratify_index=None)\n",
    "\n",
    "\n",
    "labeled_cifar = datasets.CIFAR10Labeled(root=args.cifar_root,\n",
    "                                        indicies = torch.Tensor(indicies_labeled).to(torch.long),\n",
    "                                        train=True,\n",
    "                                        transform=_transform,\n",
    "                                        target_transform=transforms.Compose([custom_transforms.ToOneHot(num_classes)]),\n",
    "                                        download=args.cifar_download,\n",
    "                                        )\n",
    "\n",
    "labeled_dataloader = data.DataLoader(\n",
    "    labeled_cifar,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4  # set the number of worker processes for loading data\n",
    ")\n",
    "\n",
    "unlabeled_cifar = datasets.CIFAR10Unlabeled(root=args.cifar_root,\n",
    "                                        indicies = torch.Tensor(indicies_unlabeled).to(torch.long),\n",
    "                                        train=True,\n",
    "                                        transform=_transform,\n",
    "                                        target_transform=transforms.Compose([custom_transforms.ToOneHot(num_classes)]),\n",
    "                                        download=args.cifar_download\n",
    "                                        )\n",
    "\n",
    "unlabeled_dataloader = data.DataLoader(\n",
    "    unlabeled_cifar,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4  \n",
    ")\n",
    "\n",
    "# From train\n",
    "# validation_cifar = datasets.CIFAR10Labeled(root=args.cifar_root,\n",
    "#                                         indicies = torch.Tensor(indicies_val).to(torch.long),\n",
    "#                                         train=True,\n",
    "#                                         transform=transforms.ToTensor(),\n",
    "#                                         target_transform=transforms.Compose([custom_transforms.ToOneHot(num_classes)]),\n",
    "#                                         download=args.cifar_download\n",
    "#                                         )\n",
    "\n",
    "# Or from validation                                       \n",
    "validation_cifar = datasets.CIFAR10Labeled(root=args.cifar_root,\n",
    "                                            train=False,\n",
    "                                            transform=_transform,\n",
    "                                            target_transform=transforms.Compose([custom_transforms.ToOneHot(num_classes)]),\n",
    "                                            download=args.cifar_download)\n",
    "\n",
    "validation_dataloader = data.DataLoader(\n",
    "    validation_cifar,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4  \n",
    ")\n",
    "\n",
    "print(\"Class distributions in sets\")\n",
    "for i in range(num_classes):\n",
    "    \n",
    "    print(f\"{i=}\",end=\"\\t\")\n",
    "    \n",
    "    for t in [t_val,t_labeled,t_unlabeled]:\n",
    "        print(f\"{np.mean(np.array(t) == i):2f}\",end=\"\\t\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"With datasets sizes: (respectively)\")\n",
    "print(len(labeled_cifar),len(unlabeled_cifar),len(validation_cifar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 1.47M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m args\u001b[39m.\u001b[39mtrain_iterations:\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# for first time\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m         ls,acc \u001b[39m=\u001b[39m wide_resnet\u001b[39m.\u001b[39;49mevaluate(model,eval_loss_fn,validation_dataloader,args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     61\u001b[0m         metrics[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(metrics[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m],ls\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     62\u001b[0m         metrics[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(metrics[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m],acc\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/mnt/datagrid/personal/hruskan1/SSL-diploma-thesis/mixmatch/examples/../wide_resnet.py:143\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(m, loss_fn, dl, device)\u001b[0m\n\u001b[1;32m    141\u001b[0m     N \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    142\u001b[0m     C \u001b[39m=\u001b[39m targets_hat\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 143\u001b[0m     targets \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mone_hot(targets\u001b[39m.\u001b[39;49mreshape(N),num_classes\u001b[39m=\u001b[39;49mC)\n\u001b[1;32m    146\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(loss_fn(targets_hat,targets))\n\u001b[1;32m    147\u001b[0m accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39margmax(targets_hat,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39margmax(targets,dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "# Transforamtion\n",
    "k1 = custom_transforms.GaussianNoiseChannelwise((0.0001, 0.0001, 0.0001))\n",
    "k2 = K.augmentation.RandomGaussianBlur((3,3),sigma=(1.,1.),p = 0.5)\n",
    "k3 = K.augmentation.RandomHorizontalFlip(p=0.5)\n",
    "k4 = K.augmentation.RandomVerticalFlip(p=0.5)\n",
    "#k5 = K.augmentation.RandomAffine([-5., 5.], [0.1, 0.1], [0.8, 1.2], [0., 0.15])\n",
    "\n",
    "img_trans = nn.ModuleList([k1,k2,k3,k4])\n",
    "mask_trans = nn.ModuleList([k2,k3,k4]) \n",
    "invert_trans  = nn.ModuleList([k3,k4])\n",
    "\n",
    "transform = custom_transforms.MyAugmentation(img_trans,mask_trans,invert_trans)\n",
    "\n",
    "\n",
    "\n",
    "# Model, optimizer and eval_function\n",
    "model = wide_resnet.WideResNet(num_classes)\n",
    "opt = torch.optim.Adam(params=model.parameters(),lr = args.lr)\n",
    "eval_loss_fn = losses.kl_divergence\n",
    "\n",
    "# Load previous checkpoint\n",
    "if args.get('load_path',None) is not None:\n",
    "    print(f\"Loading checkpoint : {args.load_path}\",file=open(logpath, 'a'), flush=True)\n",
    "    count,metrics,net,opt,net_args = utils.load_checkpoint(args.device,model,opt,args.load_path) \n",
    "    ewa_loss = metrics['train_criterion_ewa'][-1]\n",
    "else:\n",
    "    print(\"Creating new network!\",file=open(logpath, 'a'), flush=True)\n",
    "    \n",
    "    metrics = dict()\n",
    "    metrics['train_criterion'] = np.empty(0)\n",
    "    metrics['train_criterion_ewa'] = np.empty(0)\n",
    "    metrics['val_loss'] = np.empty(0)\n",
    "    metrics['val_acc'] = np.empty(0)\n",
    "    metrics['train_loss'] = np.empty(0)\n",
    "    metrics['train_acc'] = np.empty(0)\n",
    "    count = 0\n",
    "    ewa_loss = 0\n",
    "\n",
    "\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "# Preparation for training function \n",
    "\n",
    "labeled_train_iter = iter(labeled_dataloader)\n",
    "unlabeled_train_iter = iter(unlabeled_dataloader)\n",
    "\n",
    "# Use Teacher if desired\n",
    "if args.use_ema:\n",
    "    mixmatch_clf = models.Mean_Teacher(model)\n",
    "else:\n",
    "    mixmatch_clf = model\n",
    "\n",
    "model.train()\n",
    "model.to(args.device)\n",
    "\n",
    "# Iterate over index iterator until the desired number of iteration is achived\n",
    "while count < args.train_iterations:\n",
    "\n",
    "    if count == 0: # for first time\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,validation_dataloader,args.device)\n",
    "        metrics['val_loss'] = np.append(metrics['val_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['val_acc'] = np.append(metrics['val_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "        #\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,labeled_dataloader,args.device)\n",
    "        metrics['train_loss'] = np.append(metrics['train_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['train_acc'] = np.append(metrics['train_acc'],acc.detach().cpu().numpy())\n",
    "    \n",
    "    # Iterate over the end if necessary (Can be used with different sizes of dataloaders)\n",
    "    try:\n",
    "        data_l, labels = next(labeled_train_iter)\n",
    "    except:\n",
    "        labeled_train_iter = iter(labeled_dataloader)\n",
    "        data_l, labels = next(labeled_train_iter)\n",
    "\n",
    "\n",
    "    try:\n",
    "        data_u = next(unlabeled_train_iter)\n",
    "    except:\n",
    "        unlabeled_train_iter = iter(unlabeled_dataloader)\n",
    "        data_u = next(unlabeled_train_iter)\n",
    "\n",
    "    data_l = data_l.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "    data_u = data_u.to(args.device)\n",
    "\n",
    "    # Corner case (batches with different sizes, namely for iregular last batch)\n",
    "    critical_count = None\n",
    "    \n",
    "    current_batch_size = min(data_l.shape[0],data_u.shape[0])\n",
    "    \n",
    "    data_l = data_l[:current_batch_size]\n",
    "    labels = labels[:current_batch_size]\n",
    "    data_u = data_u[:current_batch_size]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        l_batch,u_batch = mixmatch.mixmatch(labeled_batch=data_l,\n",
    "                                                labels=labels,\n",
    "                                                unlabeled_batch=data_u,\n",
    "                                                clf=mixmatch_clf,\n",
    "                                                augumentation=transform,\n",
    "                                                K=args.K,\n",
    "                                                T=args.T,\n",
    "                                                device=args.device,\n",
    "                                                alpha=args.alpha\n",
    "                                                )\n",
    "        \n",
    "    x = torch.cat([l_batch[0],u_batch[0]],dim=0)\n",
    "    targets_l,targets_u = l_batch[1],u_batch[1]\n",
    "\n",
    "    # Interleave labeled and unlabeled samples between batches to obtain correct batchnorm calculation\n",
    "    x_splitted = list(torch.split(x, current_batch_size))\n",
    "    x_splitted = mixmatch.interleave(x_splitted, current_batch_size)\n",
    "    \n",
    "    # Forward \n",
    "    model.train() \n",
    "    logits = [model(x_splitted[0])]\n",
    "    for x in x_splitted[1:]:\n",
    "        logits.append(model(x))\n",
    "\n",
    "    # put interleaved samples back\n",
    "    logits = mixmatch.interleave(logits, current_batch_size)\n",
    "    logits_l = logits[0]\n",
    "    logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "    # Loss \n",
    "    # TODO: Deal with mask of valid regions of transformed images (fo affine transformation) -> remove black parts\n",
    "    if logits_l.shape != targets_l.shape:\n",
    "        print(logits_l.shape)\n",
    "        print(targets_l.shape)\n",
    "        print(f\"{count=}\")\n",
    "    loss_supervised = losses.soft_cross_entropy(logits_l,targets_l,reduction='mean')\n",
    "    loss_unsupervised = losses.mse_softmax(logits_u,targets_u,reduction='mean')\n",
    "\n",
    "    # Lx = -torch.mean(torch.sum(F.log_softmax(logits_l, dim=1) * targets_l, dim=1))\n",
    "    # Lu = torch.mean((torch.softmax(logits_u, dim=1) - targets_u)**2)\n",
    "    # print(f\"{loss_supervised=:.2f},{Lx=:.2f}\")\n",
    "    # print(f\"{loss_unsupervised=:.2f}{Lu=:.2f}\")\n",
    "\n",
    "    lam_u = ramps.linear_rampup(current = count, rampup_length = args.rampup_length) * args.lam_u\n",
    "    loss = loss_supervised + lam_u * loss_unsupervised\n",
    "\n",
    "    # SGD\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if args.use_ema: \n",
    "        mixmatch_clf.update_weights(model)\n",
    "\n",
    "    # Ewa loss\n",
    "    if (count == 0 and ewa_loss == 0):\n",
    "        ewa_loss = loss        \n",
    "    else:\n",
    "        ewa_loss = args.ewa_coef * ewa_loss + (1-args.ewa_coef) * loss\n",
    "    \n",
    "    # Save loss (every time):\n",
    "    metrics['train_criterion'] = np.append(metrics['train_criterion'],loss.detach().cpu().numpy())\n",
    "    metrics['train_criterrion_ewa'] = np.append(metrics['train_criterion'],ewa_loss.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    # Compute validation metrics if validation period \n",
    "    if (count % args.validation_period == args.validation_period-1) or (count == args.train_iterations-1): # for first time\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,validation_dataloader,args.device)\n",
    "        metrics['val_loss'] = np.append(metrics['val_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['val_acc'] = np.append(metrics['val_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "        #\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,labeled_dataloader,args.device)\n",
    "        metrics['train_loss'] = np.append(metrics['train_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['train_acc'] = np.append(metrics['train_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    # Print log if log period\n",
    "    if (count % args.log_period == args.log_period-1) or (count == args.train_iterations-1):\n",
    "        strtoprint = f\"batch iteration: {str(count)} \"+  \\\n",
    "                     f\"ewa loss: {ewa_loss:.2f} \" + \\\n",
    "                     f\"val loss: {metrics['val_loss'][-1]:.2f} \" + \\\n",
    "                     f\"val acc: {metrics['val_acc'][-1]:.2f} \" + \\\n",
    "                     f\"train loss: {metrics['train_loss'][-1]:.2f} \" + \\\n",
    "                     f\"train acc:  {metrics['train_acc'][-1]:.2f} \"\n",
    "        print(strtoprint, file=open(logpath, 'a'), flush=True)\n",
    "\n",
    "    # Save checkpoint if save_period\n",
    "    if (count % args.save_period == args.save_period-1) or (count == args.train_iterations-1):\n",
    "        m_path = model_path + f\"e{count}\" + '-m.pt'\n",
    "        print(f'# Saving Model : {m_path}', file=open(logpath, 'a'), flush=True)\n",
    "        utils.save_checkpoint(count,metrics,model,opt,args,m_path)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
