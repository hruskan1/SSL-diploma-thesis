\section{Variational Autoencoders (VAEs)}
\label{sec:vae}
The \textit{Variational Autoencoder} or VAE for short is generative model that falls into generative modelling (page \pageref{generative-modelling}). 
It is a neural network architecture that is capable of learning a low-dimensional representation of complex high-dimensional data such as 
images, text, or audio. The VAE is a probabilistic model that learns to approximate the true data distribution by using an encoder network to 
map input data into a latent space, and a decoder network to map the latent space back to the original data space. But before we delve into the details of VAE, 
let's explain the term ``Variational Autoencoders'' itself and what it actually represents. The explanation comes in two parts, first we explain 
autoencoders in subsection \ref{subsec:autoencoders} and then variational inference in subsection \ref{subsec:variational-bayes}. The experinced reader can skip those 
introductory parts and go right to subsection \ref{subsec:vaes}.

\subsection{Autoencoders}
\label{subsec:autoencoders}
An autoencoder was first introduced in the 1980s by Hinton \cite{autoencoders-1986}. However, it was not until the advent of deep learning 
and the availability of large amounts of data and computational resources in the 2000s and 2010s that autoencoders became widely used and 
achieved state-of-the-art results in a variety of task \cite{dim-reduction-ae-2006}. An autoencoder is a neural network designed to learn 
identity mapping in unsupervised manner to reconstruct the original input while compressing the information in the ``bottleneck'' layer 
to obtain a compressed representation (see \ref{fig:autoencoder}). Through this, we obtain an effecient dimensionality reduction: The low-dimensional 
latent representation can be used as an embedding vector in various application, such as search or data compression \cite{ae-blog-2018}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{autoencoder.png}
    \caption[Autoencoder]{Ilustration of autonecoder network with two networks: \textit{Encoder} and \textit{Decoder}, each parametrized by learnable parameters.
    Source \cite{ae-blog-2018}}
    \label{fig:autoencoder}
\end{figure}

\subsubsection*{Pluto alegory}
The concept of latent variables in generative modeling can be explained using Plato's Allegory of the Cave \cite{pluto-alegory}. 
In the allegory, people are confined to a cave and can only see two-dimensional shadows of three-dimensional objects projected onto a wall. 
Similarly, the objects we observe in the world may be generated by higher-level abstract concepts that we can never directly observe. 
These abstract concepts may represent properties such as color, size, and shape. Even though we never see and can not fully comprehend these 
higher-level concepts in all details, we can still reason and draw inferences about them through their manifestation in our lifes. In similar
way, we can approximate the latent representations, which encode the observed data \cite{diffusion-models-blog-2018}.  

However, in generative modeling, we typically seek to learn lower-dimensional latent representations rather than higher-dimensional ones. 
This is because attempting to learn a representation of higher dimension than the observation is often difficult without strong priors. 
Learning lower-dimensional latents can also be viewed as a form of compression, which can uncover semantically meaningful structure describing
observations.

The autoencoder's architecture is composed of two networks:
\begin{itemize}
    \item \textit{Encoder} network, which takes (high-dimensional) input $x$ and maps it into low-dimensional latent code $z$. We denote it as a
    function $g(\cdot)$ parametrized by $\phi$. It's goal is to provide dimensionality reduction, just like 
    any other approaches such as principle component analysis (PCA) or t-SNE. 
    \item \textit{Decoder} network, which takes the code $z$ and recovers the data $x$. We donte it as an function $f(\cdot)$ parametrized by $\theta$.
\end{itemize}
The parameters $\theta$ and $\phi$ are learned simultaneously using stochastic gradient descent (SGD). We encode the input, decode it, and
compute the mean squared error (MSE) loss for each feature in the batch to train the model. By minimizing this loss, we encourage the model
to produce output that is as close as possible to the input.
\begin{align*}
    x^\prime &= f_\theta(g_\phi(x)) = f_\theta(z) \\
    \mathcal{L}_{\text{MSE}} &= \frac{1}{n} \sum_{i=1}^{n} (x^{\prime}_i - x_i)^2
\end{align*}
Vanilla autoencoders can be prone to overfitting if their capacity is too high relative to the size of the dataset. To improve their
robustness, Vincent et \textit{al.} proposed a \textit{denoising} autoencoder in 2008 \cite{denoising-ae-2008}. This approach involves adding
random noise to the input and then training the model to reconstruct the original signal. By forcing the model to learn the underlying
structure of the data, rather than just memorizing the training examples, denoising autoencoders can achieve better generalization 
performance. The idea of adding noise is today known as dropout technique \cite{dropout-2014}. 

Since then, other architectures have been proposed to improve robustness and prevent overfitting. These include sparse autoencoders
\cite{sparse-ae-2011}, k-sparse autoencoders \cite{ksparse-ae-2014}, and contractive autoencoders \cite{contractive-ae-2011}. The novel 
approach was then defined in 2013 by Kingma and Welling \cite{vae-original-2013} and VAEs were introduced. The key idea was to assume that 
latent space is not deterministic, but stochastic with some distribution $p(z)$ over it. The goal of the VAE is to model this 
distribution by variational bayes. 

\subsection{Variational Calculus and Variational Bayes}
\label{subsec:variational-bayes}
The idea of autoencoder was described above, but what does the ``\textit{variational}'' in ``variational autoencoder'' stands for? 
To start with, we shall introduce the idea of \textit{variational calculus}. Then we will introduce it in the field of 
\textit{Bayesian statistics}, where it is used as a tool to overcome issuse with complex and non-trivial distributions, which we need to model.  
\subsubsection*{Variational calculus}
The variational calculus is a broad field of mathematical analysis that shares many similarities with the more familiar continuous optimization
and differential calculus techniques. In classical physics, it is also known as "Hamilton's principle" and plays a critical role in Lagrangian
mechanics \cite{lagrangian-mechanics-1998,kulhanek-2016}. The concepts of variational calculus are also extensively used in continuous optimal
control theory \cite{optimal-control-2004} and many other fields.  In this context, we will introduce the concept of variational calculus 
indirectly by defining the problem of optimization, highlighting its similarities to differential calculus.

A real-valued function $f$ defined on a domain $X$ has a global (or absolute) minimum point (minimizer) at $x^\star$, 
if $f(x^\star) \leq f(x)$ for all $x \in X$. The value of the function at a minimum point is called the \textit{minimum value} of 
the function, denoted 
$$
\min_{x \in X}(f(x)).
$$ 
If the domain $X$ is a metric space, then $f$ is said to have a \textit{local minimum point} at $x^\star$, 
if $f(x^\star) \leq f(x)$ for all $x \in X$ within distance $\epsilon$ of $x^\star$.
The set of all \textit{minimum points} is denoted as
$$
\argmin_{x\in X} = \big\{x\in X | f(x) \leq f(x^\prime)\, \forall x^\prime \in X \big\}.
$$ 
In both the global and local cases, the concept of a strict extremum can be defined: $x^\star$ is a \textit{strict local maximum point} if 
there exists some $\epsilon > 0$ such that, for all $x \in X$ within distance $\epsilon$ of $x^\star$ with $x \neq x^\star$, we have 
$f(x^\star) < f(x)$. Note that a point is a strict global minimum point if and only if it is the unique global minimum point. 

The goal of the mathematical optimization (also known as mathematical programming) is to find a minimum (or maximum) of a real valued function 
$f: X^\prime \to \mathbb{R}$ on a subset $X \subset X^\prime$ \cite{werner-opt-2022}. This is very general formulation as the $X$ can be arbitrary.
In case the $X^\prime = \mathbb{R}^n$, we talk about \textit{continuous optimization} and we use the differential calculus to obtain the solution.
If the $X^\prime$ is countable, i.e. there exists an injective function from $X^\prime$ to the set of natural numbers:
$$
X^\prime  \prec \mathbb{N} = \{1,2,3,...\},
$$
we talk about \textit{combinatorical optimization}.
Lastly, if the $X^\prime$ contains functions itself, e.g. all continous real-valued functions on closed interval $[a,b]$:
$$
X^\prime \subset C(a,b),
$$
we talk about \textit{calculus of variations}. 

In variational calculus, we call the small change in the function $x \in X^\prime$ as \textit{variation} and denote it $\delta x$
(compare it with  \textit{differential} of number $\d x$ from differential calculus). The objective function $f$ is called \textit{functional} as 
it maps functions to real numbers $\mathbb{R}$. Functionals are often expressed as definite integrals involving functions and their derivatives.
Even though it is not important for explaining the \textit{variational bayes}, we will mention Euler-Lagrange equations, which plays key role in 
variational calculus. They are system of second-order ordinary differential equations\cite{intro-variational-calc-2003}.They provide the 
stationary points (candidates for extrema) of the given functional $f$. Formaly, let 
\begin{equation*}
    J(f) = \int_a^b L(x,f(x),f^\prime(x)) \d x 
\end{equation*}
be the functional to be minimized. We are looking for a continously differentiable function\cite{smooth-functions-2023} $f \in C^1([a,b])$, which satisfies the boundary conditions $f(a) = A$ and $f(b) = B$.
We also assume that $L$ is twice continously differentiable. Function $f$ is a stationary point of $J$ if and only if
\begin{equation*}
    \frac{\partial L}{\partial f} - \frac{\d}{\d x} \frac{\partial L}{\partial f^\prime} = 0 
\end{equation*}
The derivation of Euler-Lagrange equations is straightforward and is shown in many introductory materials, such as \cite{hurak-2021} or \cite{kulhanek-2016}.

\subsubsection*{Variational Bayes}
Now we know what \textit{variations} are, but how do we apply them in the context of bayesian inference? Let's assume we have a probabilistic
graphical model (PGM) or Bayesian network \cite{graphical-models-2023} with some hidden (or unobserved) nodes $H$ and some observed nodes (evidence) $E$. The goal 
of bayesian inference is to compute posterior probability $p(H|E)$:
$$
p(H|E) = \frac{p(H,E)}{p(E)} = \frac{p(E|H) p(H)}{p(E)}
$$ 
where $p(E)$ is marginal density of the evidence:
$$
p(E) = \SumInt_{H} p(H,E)\, \d H
$$
Computing the evidence is for the most of the models intractable due to the integral or higher number of hidden variables (even if they were from
categorical distribuion). The intractability of $p(E)$ is related to the intractability of the
posterior distribution $p(H|E)$. Note that the joint distribution $p(H, E)$ is efficient to compute, and that the densities are related by Bayes formula.
Since $p(H, E)$ is tractable to compute, a tractable marginal likelihood $p(E)$ leads to a tractable posterior $p(H|E)$, and vice versa \cite{intro-vae-2019,vb-intro-1999}.
 
Variational Bayes (VB) is a technique for approximating complex probability distributions by simpler ones and was introduced by Jordan et al. in 1999 \cite{vb-intro-1999}. 
VB provides a way to approximate the posterior distribution by a simpler one that belongs to a tractable family of distributions, and to do so
by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximate one. The KL divergence is a functional with respect 
to approximate tractable distribution $q(H)$ since the true posterior $p(H|E)$ is given, hence the connection with \textit{variational calculus}.  
The KL divergence is a measure of the dissimilarity between two probability distributions:
\begin{align*}
q^\star(H) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(H) || p(H | E)) \\
\mathrm{KL}(q(H) || p(H | E)) &= \SumInt q(H) \log \big[ \frac{q(H)}{p(H|E)} \big]\,\d H,
\end{align*}
where $\mathcal{Q}$ is the family of tractable distributions, and $\mathrm{KL}$ is the KL divergence. It should be noted that there are other 
non-optimization based methods to do such approximate inference, such as MCMC \cite{wiki-mcmc-2023}. 

As the KL divergence is not symmetrical, one could ask why we have defined the optimization task as the reverse KL-divergence and not the other way
around, i.e. forward KL divergence $\mathrm{KL}(p(H | E) | q(H))$. Both cases are illustrated in figure \ref{fig:forward-reverse}. The reverse 
KL~divergence minimization results in $q$ \textit{under-estimating} $p$, which can be perceived as a safe choice. This choice ensures that sampling
from found $q$ provides values which are plausible under original $p$. For a thorough explanation, we refer to the literature 
\cite{another-vb-intro-2021}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fwd-rvrs-kl.png}
    \caption[Forward vs reverse KL divergence]{Ilustration of forward vs reverse KL-divergence on a bimodal distribution. The blue and the red contours represent the target $p$ 
    and the unimodal approximation $q$, respectively.In (a), the forward KL-divergence minimization is visulized with $q$ trying to cover up $p$.
    The (b) and (c) shows the reverse KL-divergence where $q$ locks on to one of the two modes. Source \cite{another-vb-intro-2021}}
    \label{fig:forward-reverse}
\end{figure}
We will now relabel our variables to follow
the standard notation used in deep learning literatue, where hidden variables $H$ are known as latent $\mathbf{z}$ and observed $E$ as features $\mathbf{x}$. 
In this setting, we want to optimize:
\begin{align}
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) \\
    \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) &= \SumInt_\mathbf{z} q(\mathbf{z}) \log \big[ \frac{q(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \big]\,\d \mathbf{z} \\
    &= \SumInt_\mathbf{z} \big[q(\mathbf{z}) \log q(\mathbf{z})\big] \d \mathbf{z} - \SumInt_\mathbf{z}  \big[q(\mathbf{z}) \log p(\mathbf{z}|\mathbf{x})\big] \d \mathbf{z} \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{z}|\mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \Big[\log \big( \frac{p(\mathbf{x},\mathbf{z})}{\mathbf{x}}\big) \Big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \EX_q \big[\log p(\mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \log p(\mathbf{x})
\end{align}
where the $\EX_q \big[\log p(\mathbf{x})\big] = \log p(\mathbf{x})$ because the $p(\mathbf{x})$ does not depend on $q(\mathbf{x})$. 
We can not directly optimize the KL divergence since the evidence $p(\mathbf{x})$ is intractable, however it is constant (for given datset).
If we rearange the last equation, we obtain 
\begin{align}
\log p(\mathbf{x}) - \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) &= \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \\
    &= \mathrm{ELBO}(q)
\end{align}
where the left hand side (LHS) of the equation is called evidence lower bound (ELBO), since it is truly a lower bound on the logarithm of evidence 
$p(\mathbf{x})$. This is clear to see as the KL divergnce is always positive. As the $\log p(\mathbf{x})$ is constant, maximizing the RHS is 
equal to minimzing the KL-divergence:
\begin{align*} 
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) \\
                        &= \argmax_{q \in \mathcal{Q}} \mathrm{ELBO}(q) \\
                        &= \argmax_{q \in \mathcal{Q}} \bigl[ \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \bigr]  \\
\end{align*}
A commonly made assumption regarding the variational distribution $q(\mathbf{z})$ is that it factorizes over a partition of the latent variables, 
such that the latent variables can be partitioned into subsets $\mathbf{z}_1,\dots,\mathbf{z}_m$. Specifically, we can write:
$$
q(\mathbf{z}) = \prod_{i=1}^m q_i(\mathbf{z}_i|\mathbf{x})
$$
This assumption is known as a \textit{mean field approximation}. It can be shown (see \cite{intro-variational-blog-2019}) that the following 
equation holds for optimal $q^\star_j$: 
$$
\log q^\star_j(\mathbf{z}_j|\mathbf{x}) = \EX_{i \neq j} \log p(\mathbf{x},\mathbf{z}) + \text{constant}
$$
where $\EX_{{i\neq j}}[\ln p({\mathbf {z}},{\mathbf {x}})]$ represents the expected value of the logarithm of the joint probability 
of the observed data and latent variables, taken over all variables that are not included in the current partition.The constant is 
only a normalizing constant, so $q^\star_j$ is truly a probability distribution.This leads to iterative 
algorithm of block-cordinate ascent on ELBO with initial random guess (see algorithm \ref{alg:cavi}). Furthermore, Neal and Hinton
have shown in \cite{em-as-vb-1998} that the Expectation Maximization (EM) alogrithm (proposed by Dempster et \textit{al.} \cite{em-alg-1977}) 
can be seen as such ascent on ELBO. 

\begin{algorithm}
    \caption[CAVI algorithm]{Coordinate Ascent Variational Inference (CAVI) Source: \cite{intro-variational-blog-2019}}
    \label{alg:cavi}
    \begin{algorithmic}[2]
      \State \textbf{Input:} A model $p(\mathbf{x},\mathbf{z})$, a dataset $\mathbf{x}$
      \State \textbf{Initialize:} Variational factors $q_j(\mathbf{z}_j)$
      \While{the $\mathrm{ELBO}$ has not converged}
        \For{$j \in \{1,\dots,m\}$}
          \State Set $q_j(z_j) \gets \frac{\exp{\EX_{i \neq j} [\log p(\mathbf{z},\mathbf{x})]}}{\SumInt_{\mathbf{z}_j}\exp{\EX_{i \neq j} [\log p(\mathbf{z},\mathbf{x})]}\d \mathbf{z}_j}$
        \EndFor
        \State Compute $\mathrm{ELBO}(q) \gets \EX \bigl[\log(p(\mathbf{x},\mathbf{z}))\bigr] - \EX \bigl[ \log q(\mathbf{z}) \bigr]$
      \EndWhile
      \State \textbf{return} $\prod_{j=1}^m q_j(\mathbf{z}_j)$
    \end{algorithmic}
\end{algorithm}

\subsection{VAE}
\label{subsec:vaes}
The variational autoencoders were first introduced by Kingma and Welling in 2013 \cite{vae-original-2013}.

\subsubsection{Deep Latent Variable Models}
A deep latent variable model (DLVM) is a type of probabilistic graphical model or Bayesian network where some variables are hidden or latent \cite{intro-vae-2019}.
These models use neural networks to parameterize the distributions of the its variables, enabling very complex marginal distributions (evidence)
$p_\theta(\mathbf{x})$ even if each factor in the directed model (prior $p_\theta(\mathbf{z})$ or conditional $p_\theta(\mathbf{x}|\mathbf{z})$)
is relatively simple. This expresivity maskes them attractive for approximating complicated true distributions $p^\star(\mathbf{x})$. 
The simplest DLVM is one that is factored as with following structure:
$$
p_\theta(\mathbf{x},\mathbf{z}) = p_\theta(\mathbf{z})p_\theta(\mathbf{x}|\mathbf{z})
$$
where $p_\theta(\mathbf{z})$ and/or $p_\theta(\mathbf{x}|\mathbf{z})$ are specified, i.e. their family of distribution is fixed and its distribution
parameters $\mathbf{\eta}$ are parametrized by neural network:
\begin{align*}
\mathbf{\eta} &= \mathrm{NeuralNet}_\theta(\text{Pa}(\mathbf{x})) \\
p_\theta(\mathbf{x}|\text{Pa}(\mathbf{x})) &= p_\theta(x|\mathbf{\eta})
\end{align*}
or even both their family and distribuion parameters are fixed, e.g. 
$$
p_\theta(\mathbf{z}) = p(\mathbf{z}) = \mathcal{N}(\mathbf{z},0,\mathbf{I})
$$
The $\text{Pa}(\mathbf{x}))$ corresponds to all parents of node $\mathbf{x}$ in Bayesian Network.
Such DLVM is visualized in picture \ref{fig:vae_pgm} as a Bayesian network. We will later see that 
it is exactly this model that is under consideration when talking about VAE. 

\begin{example}[DLVM for multivariate Bernoulli data]
    A simple example of DLVM for binary model $\mathbf{x} \in \{0,1\}^{D}$ used in \cite{vae-ssl-dgm-2014} has following structure:
    Latent space $\mathbf{z}$ is fixed as a spherical Gaussian distribution and conditional probability is modeled as
    a factorized Bernoulli:
    \begin{align*}
        p(\mathbf{z}) &= \mathcal{N}(\mathbf{z},0,\mathbf{I})\\
        \mathbf{p} &= \mathrm{DecoderNet}_\theta(\mathbf{z}) \\
        \log p(\mathbf{x}|\mathbf{z}) &= \sum_{i=1}^D log(x_j | \mathbf{z}) = \sum_{i=1}^D \log \mathrm{Bern}(x_i,p_i)\\
        &= \sum_{i=1}^D x_i \log p_j + (1-x_j) \log(1-p_j)\\
    \end{align*}
    where $p_j \in [0,1]\,\forall p_j \in \mathbf{p}$ and $\mathrm{Bern}(\cdot,p)$ is the probability mass function (PMF)
    of the Bernoulli distribution.
\end{example}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{vae_pgm.png}
    \caption[VAE as Bayesian network]{Ilustration of VAE as Bayesian Network. Solid lines denote generative capabilities 
    $p_\theta(\mathbf{x}|\mathbf{z})p_\theta(\mathbf{z})$, dashed lines denote the variational approximation $q_\phi(\mathbf{z}|\mathbf{x})$
    to the intractable posterior $p_\theta(\mathbf{z}|\mathbf{x})$. The variational parameters $\phi$ are learned jointly with the generative model pa-
    rameters $\theta$. Source \cite{vae-original-2013}}
    \label{fig:vae_pgm}
\end{figure}

Up until now, we have only discussed DLVMs, which involve using neural networks in Bayesian networks. However, DLVMs face the same issues as 
other Bayesian models discussed in the Variational Bayes section, such as the inability to compute the evidence $p(\mathbf{x})$ due to 
intractabilities. To overcome this, approximate inference techniques such as Expectation Maximization (EM) (see \ref{alg:cavi}) or 
Monte Carlo Markov Model (MCMC)  can be used to approximate the posterior $p_\theta(\mathbf{z}|\mathbf{x})$ and the marginal 
likelihood $p_\theta(\mathbf{x})$ \cite[appx.~A2]{intro-vae-2019}.  However, these traditional inference methods are computationally expensive,
often requiring per-datapoint optimization loops and are not well-suited for datasets with a large number of examples $N$. Moreover, they tend
to yield poor posterior approximations. Thus, there is a need for more efficient procedures.
\subsubsection{Approximate Posterior}
\label{subsubsec:vae_encoder}
To address the intractabilities of DLVM, VAEs utilize the \textit{recognition model} $q_\phi(\mathbf{z}|\mathbf{x})$, also known as an 
\textit{inference model} or an \textit{encoder}, to approximate the intractable $p_\theta(\mathbf{z}|\mathbf{x})$. While the encoder, similar 
to DLVM, can be any Bayesian network, in vanilla VAE, it is typically a simple network with $\mathbf{x}$ as input, which produces the parameter for
the distribution for the latent $\mathbf{z}$. We denote the parameters of this network with $\phi$ to differentiate them from $\theta$, and refer
to them as \textit{variational}, as the goal of the encoder is to approximate the conditional distribution $p_\theta(\mathbf{z}|\mathbf{x})$:
$$
q_\phi(\mathbf{z}|\mathbf{x}) \approx p_\theta(\mathbf{z}|\mathbf{x})
$$
It is important to note that, unlike the approximate posterior in mean-field variational inference, the encoder is not necessarily factorial, and
its parameters $\phi$ are not computed from a closed-form expectation \cite{vae-original-2013}. Instead, VAEs' framework introduces a method for 
jointly learning the recognition model parameters $\phi$ and the generative model parameters $\theta$.  This approach is known as \textit{amortized 
variational inference} \cite{amortized-inference-2014}, which avoids a per-datapoint optimization loop and leverages the efficiency of SGD \cite{intro-vae-2019}.
\subsubsection{ELBO objective}

