\section{Variational Autoencoders (VAEs)}
\label{sec:vae}
The \textit{Variational Autoencoder} or VAE for short is generative model that falls into generative modelling (page \pageref{generative-modelling}). 
It is a neural network architecture that is capable of learning a low-dimensional representation of complex high-dimensional data such as 
images, text, or audio. The VAE is a probabilistic model that learns to approximate the true data distribution by using an encoder network to 
map input data into a latent space, and a decoder network to map the latent space back to the original data space. But before we delve into the details of VAE, 
let's explain the term ``Variational Autoencoders'' itself and what it actually represents. The explanation comes in two parts, first we explain 
autoencoders in subsection \ref{subsec:autoencoders} and then variational inference in subsection \ref{subsec:variational-bayes}. The experinced reader can skip those 
introductory parts and go right to subsection \ref{subsec:vaes}.

\subsection{Autoencoders}
\label{subsec:autoencoders}
An autoencoder was first introduced in the 1980s by Hinton \cite{autoencoders-1986}. However, it was not until the advent of deep learning 
and the availability of large amounts of data and computational resources in the 2000s and 2010s that autoencoders became widely used and 
achieved state-of-the-art results in a variety of task \cite{dim-reduction-ae-2006}. An autoencoder is a neural network designed to learn 
identity mapping in unsupervised manner to reconstruct the original input while compressing the information in the ``bottleneck'' layer 
to obtain a compressed representation (see \ref{fig:autoencoder}). Through this, we obtain an effecient dimensionality reduction: The low-dimensional 
latent representation can be used as an embedding vector in various application, such as search or data compression \cite{ae-blog-2018}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{autoencoder.png}
    \caption[Autoencoder]{Ilustration of autonecoder network with two networks: \textit{Encoder} and \textit{Decoder}, each parametrized by learnable parameters.
    Source \cite{ae-blog-2018}}
    \label{fig:autoencoder}
\end{figure}

\subsubsection*{Plato alegory}
The concept of latent variables in generative modeling can be explained using Plato's Allegory of the Cave \cite{plato-alegory}. 
In the allegory, people are confined to a cave and can only see two-dimensional shadows of three-dimensional objects projected onto a wall. 
Similarly, the objects we observe in the world may be generated by higher-level abstract concepts that we can never directly observe. 
These abstract concepts may represent properties such as color, size, and shape. Even though we never see and can not fully comprehend these 
higher-level concepts in all details, we can still reason and draw inferences about them through their manifestation in our lifes. In similar
way, we can approximate the latent representations, which encode the observed data \cite{diffusion-models-blog-2018}.  

However, in generative modeling, we typically seek to learn lower-dimensional latent representations rather than higher-dimensional ones. 
This is because attempting to learn a representation of higher dimension than the observation is often difficult without strong priors. 
Learning lower-dimensional latents can also be viewed as a form of compression, which can uncover semantically meaningful structure describing
observations.

The autoencoder's architecture is composed of two networks:
\begin{itemize}
    \item \textit{Encoder} network, which takes (high-dimensional) input $\boldsymbol{x}$ and maps it into low-dimensional latent code $\boldsymbol{z}$. We denote it as a
    function $\boldsymbol{g}(\cdot)$ parametrized by $\boldsymbol{\phi}$. It's goal is to provide dimensionality reduction, just like 
    any other approaches such as principle component analysis (PCA) or t-SNE. 
    \item \textit{Decoder} network, which takes the code $\boldsymbol{z}$ and recovers the data $\boldsymbol{x}$. We donte it as an function $\boldsymbol{f}(\cdot)$ parametrized by $\boldsymbol{\theta}$.
\end{itemize}
The parameters $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ are learned simultaneously using stochastic gradient descent (SGD). We encode the input, decode it, and
compute the mean squared error (MSE) loss for each feature in the batch to train the model. By minimizing this loss, we encourage the model
to produce output that is as close as possible to the input.
\begin{align*}
    {\boldsymbol{x}}^\prime &= f_{\boldsymbol{\theta}}(\boldsymbol{g}_{\boldsymbol{\phi}}(\boldsymbol{x})) = f_{\boldsymbol{\theta}}(\boldsymbol{z}) \\
    \mathcal{L}_{\text{MSE}} &= \frac{1}{n} \sum_{i=1}^{n} (\boldsymbol{x}^{\prime}_i - \boldsymbol{x}_i)^2
\end{align*}
Vanilla autoencoders can be prone to overfitting if their capacity is too high relative to the size of the dataset. To improve their
robustness, Vincent et \textit{al.} proposed a \textit{denoising} autoencoder in 2008 \cite{denoising-ae-2008}. This approach involves adding
random noise to the input and then training the model to reconstruct the original signal. By forcing the model to learn the underlying
structure of the data, rather than just memorizing the training examples, denoising autoencoders can achieve better generalization 
performance. The idea of adding noise is today known as dropout technique \cite{dropout-2014}. 

Since then, other architectures have been proposed to improve robustness and prevent overfitting. These include sparse autoencoders
\cite{sparse-ae-2011}, k-sparse autoencoders \cite{ksparse-ae-2014}, and contractive autoencoders \cite{contractive-ae-2011}. The novel 
approach was then defined in 2013 by Kingma and Welling \cite{vae-original-2013} and VAEs were introduced. The key idea was to assume that 
latent space is not deterministic, but stochastic with some distribution $p(\boldsymbol{z})$ over it. The goal of the VAE is to model this 
distribution by variational bayes. 

\subsection{Variational Bayes}

\label{subsec:variational-bayes}
% % 
% \subsubsection*{Variational calculus}
% \todo{remove this section}
% The variational calculus is a broad field of mathematical analysis that shares many similarities with the more familiar continuous optimization
% and differential calculus techniques. In classical physics, it is also known as "Hamilton's principle" and plays a critical role in Lagrangian
% mechanics \cite{lagrangian-mechanics-1998,kulhanek-2016}. The concepts of variational calculus are also extensively used in continuous optimal
% control theory \cite{optimal-control-2004} and many other fields.  In this context, we will introduce the concept of variational calculus 
% indirectly by defining the problem of optimization, highlighting its similarities to differential calculus.

% A real-valued function $f$ defined on a domain $X$ has a global (or absolute) minimum point (minimizer) at $x^\star$, 
% if $f(x^\star) \leq f(x)$ for all $x \in X$. The value of the function at a minimum point is called the \textit{minimum value} of 
% the function, denoted 
% $$
% \min_{x \in X}(f(x)).
% $$ 
% If the domain $X$ is a metric space, then $f$ is said to have a \textit{local minimum point} at $x^\star$, 
% if $f(x^\star) \leq f(x)$ for all $x \in X$ within distance $\epsilon$ of $x^\star$.
% The set of all \textit{minimum points} is denoted as
% $$
% \argmin_{x\in X} = \big\{x\in X \mid f(x) \leq f(x^\prime)\, \forall x^\prime \in X \big\}.
% $$ 
% In both the global and local cases, the concept of a strict extremum can be defined: $x^\star$ is a \textit{strict local maximum point} if 
% there exists some $\epsilon > 0$ such that, for all $x \in X$ within distance $\epsilon$ of $x^\star$ with $x \neq x^\star$, we have 
% $f(x^\star) < f(x)$. Note that a point is a strict global minimum point if and only if it is the unique global minimum point. 

% The goal of the mathematical optimization (also known as mathematical programming) is to find a minimum (or maximum) of a real valued function 
% $f: X^\prime \to \mathbb{R}$ on a subset $X \subset X^\prime$ \cite{werner-opt-2022}. This is very general formulation as the $X$ can be arbitrary.
% In case the $X^\prime = \mathbb{R}^n$, we talk about \textit{continuous optimization} and we use the differential calculus to obtain the solution.
% If the $X^\prime$ is countable, i.e. there exists an injective function from $X^\prime$ to the set of natural numbers:
% $$
% X^\prime  \prec \mathbb{N} = \{1,2,3,...\},
% $$
% we talk about \textit{combinatorical optimization}.
% Lastly, if the $X^\prime$ contains functions itself, e.g. all continous real-valued functions on closed interval $[a,b]$:
% $$
% X^\prime \subset C(a,b),
% $$
% we talk about \textit{calculus of variations}. 

% In variational calculus, we call the small change in the function $x \in X^\prime$ as \textit{variation} and denote it $\delta x$
% (compare it with  \textit{differential} of number $\d x$ from differential calculus). The objective function $f$ is called \textit{functional} as 
% it maps functions to real numbers $\mathbb{R}$. Functionals are often expressed as definite integrals involving functions and their derivatives.
% Even though it is not important for explaining the \textit{variational bayes}, we will mention Euler-Lagrange equations, which plays key role in 
% variational calculus. They are system of second-order ordinary differential equations\cite{intro-variational-calc-2003}.They provide the 
% stationary points (candidates for extrema) of the given functional $f$. Formaly, let 
% \begin{equation*}
%     J(f) = \int_a^b L(x,f(x),f^\prime(x)) \d x 
% \end{equation*}
% be the functional to be minimized. We are looking for a continously differentiable function\cite{smooth-functions-2023} $f \in C^1([a,b])$, which satisfies the boundary conditions $f(a) = A$ and $f(b) = B$.
% We also assume that $L$ is twice continously differentiable. Function $f$ is a stationary point of $J$ if and only if
% \begin{equation*}
%     \frac{\partial L}{\partial f} - \frac{\d}{\d x} \frac{\partial L}{\partial f^\prime} = 0 
% \end{equation*}
% The derivation of Euler-Lagrange equations is straightforward and is shown in many introductory materials, such as \cite{hurak-2021} or \cite{kulhanek-2016}.
% %


The idea of autoencoder was described above, but what does the ``\textit{variational}'' in ``variational autoencoder'' stands for? 
Calculus of variations is a branch of mathematics that deals with finding the optimal solution of a functional, which is a mathematical 
function that maps a set of functions to real numbers. The optimal solution is the function that minimizes or maximizes the functional. 
The calculus of variations has many applications in physics \cite{lagrangian-mechanics-1998}, engineering \cite{optimal-control-2004}, economics, and other fields.
Its development dates back to the 17th century. The Euler-Lagrange equations \cite{intro-variational-calc-2003} play a crucial role in variational calculus by providing the stationary points of a given 
functional that needs to be minimized or maximized.

Now we know what \textit{variations} are, but how do we apply them in the context of bayesian inference? Let's assume we have a probabilistic
graphical model (PGM) or Bayesian network \cite{graphical-models-2023} with some hidden (or unobserved) nodes $H$ and some observed nodes (evidence) $E$. The goal 
of bayesian inference is to compute posterior probability $p(H\mid E)$:
$$
p(H \mid E) = \frac{p(H,E)}{p(E)} = \frac{p(E\mid H) p(H)}{p(E)}
$$ 
where $p(E)$ is marginal density of the evidence:
$$
p(E) = \int_{H} p(H,E)\, \d H
$$
Computing the evidence is for the most of the models intractable due to the integral or higher number of hidden variables (even if they were from
categorical distribuion). The intractability of $p(E)$ is related to the intractability of the
posterior distribution $p(H \mid E)$. Note that the joint distribution $p(H, E)$ is efficient to compute, and that the densities are related by Bayes formula.
Since $p(H, E)$ is tractable to compute, a tractable marginal likelihood $p(E)$ leads to a tractable posterior $p(H \mid E)$, and vice versa \cite{intro-vae-2019,vb-intro-1999}.
 
Variational Bayes (VB) is a technique for approximating complex probability distributions by simpler ones and was introduced by Jordan et al. in 1999 \cite{vb-intro-1999}. 
VB provides a way to approximate the posterior distribution by a simpler one that belongs to a tractable family of distributions, and to do so
by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximate one. The KL divergence is a functional with respect 
to approximate tractable distribution $q(H)$ since the true posterior $p(H \mid E)$ is given, hence the connection with \textit{variational calculus}.  
The KL divergence is a measure of the dissimilarity between two probability distributions:
\begin{align*}
q^\star(H) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(H) || p(H  \mid  E)) \\
\mathrm{KL}(q(H) || p(H \mid E)) &= \int q(H) \log \big[ \frac{q(H)}{p(H \mid E)} \big]\,\d H,
\end{align*}
where $\mathcal{Q}$ is the family of tractable distributions, and $\mathrm{KL}$ is the KL divergence. It should be noted that there are other 
non-optimization based methods to do such approximate inference, such as MCMC \cite{wiki-mcmc-2023}. 

As the KL divergence is not symmetrical, one could ask why we have defined the optimization task as the reverse KL-divergence and not the other way
around, i.e. forward KL divergence $\mathrm{KL}(p(H \mid E) \mid q(H))$. Both cases are illustrated in figure \ref{fig:forward-reverse}. The reverse 
KL~divergence minimization results in $q$ \textit{under-estimating} $p$, which can be perceived as a safe choice. This choice ensures that sampling
from found $q$ provides values which are plausible under original $p$. For a thorough explanation, we refer to the literature 
\cite{another-vb-intro-2021}.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fwd-rvrs-kl.png}
    \caption[Forward vs reverse KL divergence]{Ilustration of forward vs reverse KL-divergence on a bimodal distribution. The blue and the red contours represent the target $p$ 
    and the unimodal approximation $q$, respectively.In (a), the forward KL-divergence minimization is visulized with $q$ trying to cover up $p$.
    The (b) and (c) shows the reverse KL-divergence where $q$ locks on to one of the two modes. Source \cite{another-vb-intro-2021}}
    \label{fig:forward-reverse}
\end{figure}
We will now relabel our variables to follow
the standard notation used in deep learning literatue, where hidden variables $H$ are known as latent $\mathbf{z}$ and observed $E$ as features $\mathbf{x}$. 
In this setting, we want to optimize:
\begin{align}
    \label{eq:variational_bayes}
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} \mid \mathbf{x})) \\
    \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} \mid \mathbf{x})) &= \int_\mathbf{z} q(\mathbf{z}) \log \big[ \frac{q(\mathbf{z})}{p(\mathbf{z}\mid\mathbf{x})} \big]\,\d \mathbf{z} \\
    &= \int_\mathbf{z} \big[q(\mathbf{z}) \log q(\mathbf{z})\big] \d \mathbf{z} - \int_\mathbf{z}  \big[q(\mathbf{z}) \log p(\mathbf{z}\mid \mathbf{x})\big] \d \mathbf{z} \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{z}\mid \mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \Big[\log \big( \frac{p(\mathbf{x},\mathbf{z})}{\mathbf{x}}\big) \Big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \EX_q \big[\log p(\mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \log p(\mathbf{x})
\end{align}
where the $\EX_q \big[\log p(\mathbf{x})\big] = \log p(\mathbf{x})$ because the $p(\mathbf{x})$ does not depend on $q(\mathbf{x})$. 
We can not directly optimize the KL divergence since the evidence $p(\mathbf{x})$ is intractable, however it is constant (for given datset).
If we rearange the last equation, we obtain 
\begin{align}
\label{eq:elbo}
\log p(\mathbf{x}) - \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} \mid  \mathbf{x})) &= \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \\
    &= \mathrm{ELBO}(q) 
\end{align}
where the left hand side (lhs) of the equation is called evidence lower bound (ELBO), since it is truly a lower bound on the logarithm of evidence 
$p(\mathbf{x})$. This is clear to see as the KL divergnce is always positive. As the $\log p(\mathbf{x})$ is constant, maximizing the rhs is 
equal to minimzing the KL-divergence:
\begin{align*} 
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} \mid  \mathbf{x})) \\
                        &= \argmax_{q \in \mathcal{Q}} \mathrm{ELBO}(q) \\
                        &= \argmax_{q \in \mathcal{Q}} \bigl[ \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \bigr]  \\
\end{align*}
A commonly made assumption regarding the variational distribution $q(\mathbf{z})$ is that it factorizes over a partition of the latent variables, 
such that the latent variables can be partitioned into subsets $\mathbf{z}_1,\dots,\mathbf{z}_m$. Specifically, we can write:
$$
q(\mathbf{z}) = \prod_{i=1}^m q_i(\mathbf{z}_i\mid \mathbf{x})
$$
This assumption is known as a \textit{mean field approximation}. It can be shown (see \cite{intro-variational-blog-2019}) that the following 
equation holds for optimal $q^\star_j$: 
$$
\log q^\star_j(\mathbf{z}_j\mid \mathbf{x}) = \EX_{i \neq j} \log p(\mathbf{x},\mathbf{z}) + \text{constant}
$$
where $\EX_{{i\neq j}}[\log p({\mathbf {z}},{\mathbf {x}})]$ represents the expected value of the logarithm of the joint probability 
of the observed data and latent variables, taken over all variables that are not included in the current partition.The constant is 
only a normalizing constant, so $q^\star_j$ is truly a probability distribution.This leads to iterative 
algorithm of block-cordinate ascent on ELBO with initial random guess (see algorithm \ref{alg:cavi}). Furthermore, Neal and Hinton
have shown in \cite{em-as-vb-1998} that the Expectation Maximization (EM) alogrithm (proposed by Dempster et \textit{al.} \cite{em-alg-1977}) 
can be seen as such ascent on ELBO. 

\begin{algorithm}
    \caption[CAVI algorithm]{Coordinate Ascent Variational Inference (CAVI) Source: \cite{intro-variational-blog-2019}}
    \label{alg:cavi}
    \begin{algorithmic}[2]
      \State \textbf{Input:} A model $p(\mathbf{x},\mathbf{z})$, a dataset $\mathbf{x}$
      \State \textbf{Initialize:} Variational factors $q_j(\mathbf{z}_j)$
      \While{the $\mathrm{ELBO}$ has not converged}
        \For{$j \in \{1,\dots,m\}$}
          \State Set $q_j(z_j) \gets \frac{\exp{\EX_{i \neq j} [\log p(\mathbf{z},\mathbf{x})]}}{\int_{\mathbf{z}_j}\exp{\EX_{i \neq j} [\log p(\mathbf{z},\mathbf{x})]}\d \mathbf{z}_j}$
        \EndFor
        \State Compute $\mathrm{ELBO}(q) \gets \EX \bigl[\log(p(\mathbf{x},\mathbf{z}))\bigr] - \EX \bigl[ \log q(\mathbf{z}) \bigr]$
      \EndWhile
      \State \textbf{return} $\prod_{j=1}^m q_j(\mathbf{z}_j)$
    \end{algorithmic}
\end{algorithm}

\subsection{VAE}
\label{subsec:vaes}
The variational autoencoders were first introduced by Kingma and Welling in 2013 \cite{vae-original-2013}. Since its introduction, the VAE 
has been widely used in research and industry, and has inspired many other generative models. It continues to be an active area of research, with 
ongoing efforts to improve its performance and applicability to different domains. This subsection was mainly inspired by \cite{intro-vae-2019}.

\subsubsection{Deep Latent Variable Models}
A deep latent variable model (DLVM) is a type of probabilistic graphical model or Bayesian network where some variables are hidden or latent \cite{intro-vae-2019}.
These models use neural networks to parameterize the distributions of the its variables, enabling very complex marginal distributions (evidence)
$p_{\boldsymbol{\theta}}(\mathbf{x})$ even if each factor in the directed model (prior $p_{\boldsymbol{\theta}}(\mathbf{z})$ or conditional $p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z})$)
is relatively simple. This expresivity maskes them attractive for approximating complicated true distributions $p^\star(\mathbf{x})$. 
The simplest DLVM is one that is factored as with following structure:
$$
p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z}) = p_{\boldsymbol{\theta}}(\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z})
$$
where $p_{\boldsymbol{\theta}}(\mathbf{z})$ and/or $p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z})$ are specified, i.e. their family of distribution is fixed and its distribution
parameters $\boldsymbol{\eta}$ are parametrized by neural network:
\begin{align*}
\boldsymbol{\eta} &= \mathrm{NeuralNet}_{\boldsymbol{\theta}}(\text{Pa}(\mathbf{x})) \\
p_{\boldsymbol{\theta}}(\mathbf{x}\mid \text{Pa}(\mathbf{x})) &= p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid \boldsymbol{\eta})
\end{align*}
or even both their family and distribuion parameters are fixed, e.g. 
$$
p_{\boldsymbol{\theta}}(\mathbf{z}) = p(\mathbf{z}) = \mathcal{N}(\mathbf{z},0,\mathbf{I})
$$
The $\text{Pa}(\mathbf{x}))$ corresponds to all parents of node $\mathbf{x}$ in Bayesian Network.
Such DLVM is visualized in picture \ref{fig:vae_pgm} as a Bayesian network. We will later see that 
it is exactly this model that is under consideration when talking about VAE. 

\begin{example}[DLVM for multivariate Bernoulli data]
    A simple example of DLVM for binary model $\mathbf{x} \in \{0,1\}^{D}$ used in \cite{vae-ssl-dgm-2014} has following structure:
    Latent space $\mathbf{z}$ is fixed as a spherical Gaussian distribution and conditional probability is modeled as
    a factorized Bernoulli:
    \begin{align*}
        p(\mathbf{z}) &= \mathcal{N}(\mathbf{z},0,\mathbf{I})\\
        \mathbf{p} &= \mathrm{DecoderNet}_{\boldsymbol{\theta}}(\mathbf{z}) \\
        \log p(\mathbf{x}\mid \mathbf{z}) &= \sum_{i=1}^D log(x_j \mid  \mathbf{z}) = \sum_{i=1}^D \log \mathrm{Bern}(x_i,p_i)\\
        &= \sum_{i=1}^D x_i \log p_j + (1-x_j) \log(1-p_j)\\
    \end{align*}
    where $p_j \in [0,1]\,\forall p_j \in \mathbf{p}$ and $\mathrm{Bern}(\cdot,p)$ is the probability mass function (PMF)
    of the Bernoulli distribution.
\end{example}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{vae_pgm.png}
    \caption[VAE as Bayesian network]{Ilustration of VAE as Bayesian Network. Solid lines denote generative capabilities 
    $p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})$, dashed lines denote the variational approximation $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$
    to the intractable posterior $p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})$. The variational parameters ${\boldsymbol{\phi}}$ are learned jointly with the generative model pa-
    rameters ${\boldsymbol{\theta}}$. Source \cite{vae-original-2013}}
    \label{fig:vae_pgm}
\end{figure}

Up until now, we have only discussed DLVMs, which involve using neural networks in Bayesian networks. However, DLVMs face the same issues as 
other Bayesian models discussed in the Variational Bayes section, such as the inability to compute the evidence $p(\mathbf{x})$ due to 
intractabilities. To overcome this, approximate inference techniques such as Expectation Maximization (EM) (see \ref{alg:cavi}) or 
Monte Carlo Markov Model (MCMC)  can be used to approximate the posterior $p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})$ and the marginal 
likelihood $p_{\boldsymbol{\theta}}(\mathbf{x})$ \cite[appx.~A2]{intro-vae-2019}.  However, these traditional inference methods are computationally expensive,
often requiring per-datapoint optimization loops and are not well-suited for datasets with a large number of examples $N$. Moreover, they tend
to yield poor posterior approximations. Thus, there is a need for more efficient procedures.

\subsubsection{Approximate Posterior}
\label{subsubsec:vae_encoder}
The VAEs frameworks introduces two models, encoder and decoder (see figure \ref{fig:vae_pgm} ) and also propose an computationally effecient algorithm for optimizing the parameters jointly
using SGD.

To address the intractabilities of DLVM, VAEs utilize the \textit{recognition model} $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$, also known as an 
\textit{inference model} or an \textit{encoder}, to approximate the intractable $p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})$. 
While the encoder, similar 
to DLVM, can be any Bayesian network, in vanilla VAE, it is typically a simple network with $\mathbf{x}$ as input, which produces the parameter for
the distribution for the latent $\mathbf{z}$. We denote the parameters of this network with ${\boldsymbol{\phi}}$ to differentiate them from ${\boldsymbol{\theta}}$, and refer
to them as \textit{variational}, as the goal of the encoder is to approximate the conditional distribution $p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})$:
$$
q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) \approx p_{\boldsymbol{\theta}}(\mathbf{z}\mid \mathbf{x})
$$
It is important to note that, unlike the approximate posterior in mean-field variational inference, the encoder is not necessarily factorial, and
its parameters ${\boldsymbol{\phi}}$ are not computed from a closed-form expectation \cite{vae-original-2013}. Instead, VAEs' framework introduces a method for 
jointly learning the recognition model parameters ${\boldsymbol{\phi}}$ and the generative model parameters ${\boldsymbol{\theta}}$.  This approach is known as \textit{amortized 
variational inference} \cite{amortized-inference-2014}, which avoids a per-datapoint optimization loop and leverages the efficiency of SGD \cite{intro-vae-2019}.
\subsubsection{ELBO objective}
The objective to minimize is an ELBO (defined in \ref{eq:elbo}), like in any other bayes variational methods. For any selection of recognition model $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$,
we can write:
\begin{align}
    \label{eq:vae}
    \log p_{\boldsymbol{\theta}}(\mathbf{x})  &= \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})\big] - \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})\big] &+ \mathrm{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) || p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x})) \\
    \log p_{\boldsymbol{\theta}}(\mathbf{x})  &= \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})) &+ \mathrm{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) || p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x}))
\end{align}
This equation provides important insight into the divergence $\mathrm{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) || p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x}))$. 
Firstly, it measures how well the approximating distribution $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$ fits the intractable distribution of decoder 
model $p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x})$. Secondly, it quantifies the gap between the ELBO and the log likelihood of the marginal likelihood 
$\log p_{\boldsymbol{\theta}}(\mathbf{x})$, also knwon as \textit{tightness} of the bound. A tighter bound indicates a better fit between the approximate 
and model posterior distributions. Therefore maximizing the $\mathrm{ELBO}$ w.r.t to ${\boldsymbol{\theta}}$ and ${\boldsymbol{\phi}}$ can be seen as a concurent task of 
improving the log evidence $p_{\boldsymbol{\theta}}(x)$ under the model and also improving the fit of variational $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$ to 
$p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x}))$:
\begin{align}
    \label{eq:opt_vae}
    \argmax_{{\boldsymbol{\theta}},{\boldsymbol{\phi}}} \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})) &= \argmax_{{\boldsymbol{\theta}},{\boldsymbol{\phi}}} \bigr[ \log p_{\boldsymbol{\theta}}(\mathbf{x}) - \mathrm{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) || p_{\boldsymbol{\theta}}(\mathbf{z} \mid  \mathbf{x})) \bigl] 
\end{align}
In order to use SGD, we need to compute the derivatives of the $\mathrm{ELBO}$ objective with respect to the model parameters ${\boldsymbol{\theta}}$ and ${\boldsymbol{\phi}}$. 
The $\mathrm{ELBO}$ objective of a (mini)-batch is defined as the sum of terms for each data point in the (mini)-batch $\mathcal{B}$:
$$
\mathrm{ELBO}(\mathcal{B}) = \sum_{\mathbf{x} \in \mathcal{B}} \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}))
$$
where the data point term is defined as
$$
\mathrm{ELBO}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})) = \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})\big] - \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})\big].
$$

This term depends on the data point $\mathbf{x}$ and the network parameters ${\boldsymbol{\theta}}$ and ${\boldsymbol{\phi}}$ that correspond to the distributions $p_{\boldsymbol{\theta}}$ and $q_{\boldsymbol{\phi}}$, respectively.
The ELBO is sometimes rearanged into:
\begin{align}
    \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})) &= \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x}\mid \mathbf{z}) + \log p_{\boldsymbol{\theta}}(\mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})\big] \notag\\
    &= \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x}\mid\mathbf{z}) \big] - \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})} \big[  \log \frac{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})}{p_{\boldsymbol{\theta}}(\mathbf{z})} \big] \notag\\
    &= \underbrace{\EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x}\mid\mathbf{z}) \big]}_{\text{recognition term}} - \underbrace{\mathrm{KL}(q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x}) ||p_{\boldsymbol{\theta}}(\mathbf{z}))}_{\text{variational regularization term}} \label{eq:elbo_terms}
\end{align}
Although the individual data point $\mathrm{ELBO}$ and its gradient $\nabla_{{\boldsymbol{\theta}},{\boldsymbol{\phi}}}\,\mathrm{ELBO}(\mathrm{x},{\boldsymbol{\theta}},{\boldsymbol{\phi}})$ are generally not tractable, VAEs provide
means and assumptions under which we can perform minibatch SGD. Namely we will show that good unbiased estimators $\hat{\nabla}_{{\boldsymbol{\theta}},{\boldsymbol{\phi}}}\,\mathrm{ELBO}(\mathrm{x},{\boldsymbol{\theta}},{\boldsymbol{\phi}})$ 
exist under the assumption that we know how to reparametrize the $q_{\boldsymbol{\phi}}$ distribuion to remove the ${\boldsymbol{\phi}}$ influence on sampling. This is also known as \textit{reparametrization trick}
and is a key achievment of VAEs frameworks. 

Say we want to compute gradient w.r.t. to decoder parameters ${\boldsymbol{\theta}}$, i.e. 
\begin{align}
    \nabla_{{\boldsymbol{\theta}}}\,\mathrm{ELBO}(\mathbf{x},{\boldsymbol{\theta}},{\boldsymbol{\phi}}) &= \nabla_{{\boldsymbol{\theta}}} \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})\big] \notag \\
        &= \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})} \nabla_{{\boldsymbol{\theta}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})\big] \label{eq:sec_line} \\
        &\simeq  \nabla_{{\boldsymbol{\theta}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\tilde{\mathbf{z}}) - \log q_{\boldsymbol{\phi}}(\boldsymbol{\tilde{z}}\mid\mathbf{x})\big] \notag\\
        &\simeq \nabla_{{\boldsymbol{\theta}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\tilde{\mathbf{z}})\big] \label{eq:monte_carlo}
\end{align}
The $\simeq$ symbol means that one of the two sides is an unbiased estimator of the other side. The rhs is a random variable with some source of the noise and two sides are equal
when averaged over the noise distribuion. In this case, the noise distribution is $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$ and $\tilde{\mathbf{z}}$ is sampled from it. In other words, the last 
line (eq. \ref{eq:monte_carlo}) is a Monte Carlo estimator of the second line (eq. \ref{eq:sec_line}) \cite{intro-vae-2019}.

The unbiased gradient w.r.t. to the encoder parameters ${\boldsymbol{\theta}}$ is slightly difficult to obtain as the expectation of the $\mathrm{ELBO}$ is taken w.r.t. the variational distribuion 
$q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$, which is a function of ${\boldsymbol{\phi}}$, i.e.
\begin{align}
    \nabla_{{\boldsymbol{\phi}}}\,\mathrm{ELBO}(\mathbf{x},{\boldsymbol{\theta}},{\boldsymbol{\phi}}) &= \nabla_{{\boldsymbol{\phi}}} \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})\big] \\
                                                         &\neq  \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \nabla_{{\boldsymbol{\phi}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z}) - \log q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})\big]
\end{align}
Nevertheless, in case of continuous latent variables, we can use \textit{reparametrization trick} \cite{renedze-backprop-2014} as we will show now \cite{intro-vae-2019}.

\subsubsection{Reparametrization trick}
By parametrizing the random variable $\mathbf{z} \sim q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$ as a differentiable and invertible mapping of another random variable $\boldsymbol{\epsilon}$ that is independent
of both the data $\mathbf{x}$ and the variational parameters ${\boldsymbol{\phi}}$, we can obtain an unbiased gradient and differentiate the evidence lower bound (ELBO) objective with respect to ${\boldsymbol{\phi}}$. 
Specifically, we have 
\begin{equation*}
    \mathbf{z} = \mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x}),
\end{equation*}
where $\boldsymbol{\epsilon}$ is randomly sampled from some distribution $p(\boldsymbol{\epsilon})$. Using this mapping, we can separate the source of noise from the variational parameters and 
obtain the Monte Carlo estimator of the gradient. i.e.
\begin{align*}
    \nabla_{{\boldsymbol{\phi}}} \EX_{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})} \big[\log \frac{p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})}\big] 
    &= \nabla_{{\boldsymbol{\phi}}} \EX_{p(\boldsymbol{\epsilon})} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x})) - \log q_{\boldsymbol{\phi}}(\mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x})\mid \mathbf{x})\big]\\
    &=  \EX_{p(\boldsymbol{\epsilon})} \nabla_{{\boldsymbol{\phi}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x})) - \log q_{\boldsymbol{\phi}}(\mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x})\mid \mathbf{x})\big] \\
    &\simeq \nabla_{{\boldsymbol{\phi}}} \big[\log p_{\boldsymbol{\theta}}(\mathbf{x},\tilde{\mathbf{z}}) - \log q_{\boldsymbol{\phi}}(\tilde{\mathbf{z}}\mid \mathbf{x})\big],
\end{align*}
where $\tilde{\mathbf{z}} = \mathbf{g}(\tilde{\boldsymbol{\epsilon}},{\boldsymbol{\phi}},\mathbf{x})$  for randomly sampled $\tilde{\boldsymbol{\epsilon}} \sim p(\boldsymbol{\epsilon})$. 
With this procedure, we can now differentiate the ELBO objective with respect to ${\boldsymbol{\phi}}$, which was not possible before, the situation is visualized in figure \ref{fig:reparam_trick}. The limitations of this procedures are, that we have to know the mapping $\mathbf{g}$.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{reparametrization_trick.png}
    \caption[Reparametrization trick]{Ilustration of reparametrization trick. The encoder parameters ${\boldsymbol{\phi}}$ affect the objective $f$ indirectly through the random variable $\mathbf{z} \sim  q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$.
    We can not compte the gradient $\nabla_{\boldsymbol{\phi}} f$ in the original setting (left), as we can not propagate the dependency of ${\boldsymbol{\phi}}$ through the sampling process. Thanks to parameterization trick, we 
    can first sample the $\epsilon$ and then compute $\mathbf{g}(\boldsymbol{\epsilon},{\boldsymbol{\phi}},\mathbf{x})$ which enables us to compute the gradients $\nabla_{\boldsymbol{\phi}} f$.
    Source \cite{intro-vae-2019}}
    \label{fig:reparam_trick}
\end{figure}
For the Gaussian distribuion the particular mapping is the following:
$$
\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\epsilon} \odot \boldsymbol{\sigma}
$$
where $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ are the mean and standard deviation of the Gaussian distribution, respectively, 
$\boldsymbol{\epsilon}$ is a vector of independent samples from a standard normal distribution, and $\odot$ denotes element-wise 
multiplication. The final algorithm for computing SGD for the ELBO loss is provided in \ref{alg:sgd}.
\begin{algorithm}[H]
    \caption[SGD for VAE]{Stochastic gradient descent for VAE. (a.k.a. Auto-Encoding Variational Bayes (AEVB) Algorithm )}
    \label{alg:sgd}
    \begin{algorithmic}[1]
    \State \textbf{Input}: Dataset $\mathcal{D}$, Inference model $q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x})$, Generative model $p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})$
    \State \textbf{Return}: Learned parameters $\boldsymbol{\theta}, \boldsymbol{\phi}$
    \State $(\boldsymbol{\theta}, \boldsymbol{\phi}) \leftarrow$ Initialize parameters
    \While{SGD not converged}
    \State $M \sim \mathcal{D}$ \Comment{Random minibatch of data}
    \State $\tilde{L}_{\theta,\phi} = 0$
    \For{$\boldsymbol{m}$ \textbf{in} $M$}
        \State Sample $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$ 
        \State Apply encoder $\boldsymbol{\eta} = \mathrm{NeuralNet}_{\boldsymbol{\phi}}(\boldsymbol{m})$
        \State Reparametrize to obtain sample $\tilde{\boldsymbol{z}}$ = $\boldsymbol{g}(\boldsymbol{\eta},\boldsymbol{m},\boldsymbol{\epsilon})$ \Comment{Via reparametrization trick}
        \State Apply decoder on $\tilde{\boldsymbol{z}}$ and compute $\log(p_{\boldsymbol{\theta}}(\mathbf{x}=\boldsymbol{m}\mid \mathbf{z}=\tilde{\boldsymbol{z}})) + p_{\boldsymbol{\theta}}(\boldsymbol{z}=\tilde{\boldsymbol{z}})$
        \State Compute negative log: $-\log q_{\boldsymbol{\phi}}(\boldsymbol{z}=\tilde{\boldsymbol{z}}\mid \boldsymbol{x}=\boldsymbol{m})$ \Comment{See subsection \ref{subsubsec:log_q}}
        \State $\tilde{L}_{\theta,\phi} \mathrel{+}= \log(p_{\boldsymbol{\theta}}(\mathbf{x}=\boldsymbol{m},\mathbf{z}=\tilde{\boldsymbol{z}})) - \log q_{\boldsymbol{\phi}}(\boldsymbol{z}=\tilde{\boldsymbol{z}}\mid \boldsymbol{x}=\boldsymbol{m})$ \Comment{Accumulate loss}
    \EndFor
    \State Compute gradients $\nabla_{\theta,\phi} \,\tilde{L}_{\theta,\phi}(M, \boldsymbol{\epsilon})$
    \State Update $\theta$ and $\phi$ using SGD optimizer
    \EndWhile
    \end{algorithmic}
\end{algorithm}

\subsubsection{Evaluation of $\log_{\boldsymbol{\phi}}(\boldsymbol{z}\mid \boldsymbol{x})$}
\label{subsubsec:log_q}
The computation of the estimator of the ELBO requires computation of the density log: (see algorithm \ref{alg:sgd})
$$
\log_{\boldsymbol{\phi}}(\boldsymbol{z}=\tilde{\boldsymbol{z}}\mid \boldsymbol{x}=\boldsymbol{m}).
$$ 
The $\tilde{\boldsymbol{z}}$ is reparametrized through the \textit{reparametrization trick}
$$
\tilde{\boldsymbol{z}} = \mathbf{g}(\tilde{\boldsymbol{\epsilon}},{\boldsymbol{\phi}},\mathbf{x}),\quad \tilde{\boldsymbol{\epsilon}} \sim p(\boldsymbol{\epsilon})
$$
and as long as the $\mathbf{g}(\cdot)$ is invertible, we can express the density of $\boldsymbol{z}$ through the density of $\boldsymbol{\epsilon}$:
$$
q_{\boldsymbol{\phi}}(\mathbf{z}\mid \mathbf{x}) = \log p(\boldsymbol{\epsilon}) - \log d_{\boldsymbol{\phi}}(\boldsymbol{x},\boldsymbol{\epsilon})
$$
where $d_{\boldsymbol{\phi}}(\boldsymbol{x},\boldsymbol{\epsilon})$ is the log of the absolute value of the determinant of the Jacobian 
matrix \cite{intro-vae-2019}:
\begin{align*}
d_{\boldsymbol{\phi}}(\boldsymbol{x},\boldsymbol{\epsilon}) &= \log |\det J(\boldsymbol{z}, \boldsymbol{\epsilon})|\\
J(\boldsymbol{z}, \boldsymbol{\epsilon}) = \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{\epsilon}} &=
\begin{bmatrix}
\frac{\partial z_1}{\partial \epsilon_1} & \frac{\partial z_1}{\partial \epsilon_2} & \cdots & \frac{\partial z_1}{\partial \epsilon_k} \\
\frac{\partial z_2}{\partial \epsilon_1} & \frac{\partial z_2}{\partial \epsilon_2} & \cdots & \frac{\partial z_2}{\partial \epsilon_k} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial z_k}{\partial \epsilon_1} & \frac{\partial z_k}{\partial \epsilon_2} & \cdots & \frac{\partial z_k}{\partial \epsilon_k}
\end{bmatrix}
\end{align*}
One can create a very flexible transformations $\boldsymbol{g}(\cdot)$ which results in more flexible inference model 
$\log_{\boldsymbol{\phi}}(\boldsymbol{z}\mid\boldsymbol{x})$ improving the capacity of the model. Between those transformations
belong factorized Gaussian posterior \cite{intro-vae-2019}, Normalizing flows (NF) \cite{nf-2015} or Inverse autoregresive transformations and 
Inverse autoregresive flows (IAF) \cite{iaf-2016}. Other possibility is to extend the latent space with auxiliary latent variables
\cite{aux-var-2016} which leads to hiearchical variational autoencoders (HVAE) firstly introduced in \cite{lvae-2016}.

\subsection{Issues with VAEs}
As every method, the vanilla VAEs have also its weak points, which were identified and posses an open question with active research:
\begin{itemize}
    \item \textbf{Bluriness of the images}: If the VAE model does not have the enough capacity to model the underlying true distribution properly,
    the variance of decoder $p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid \boldsymbol{z})$ will end up larger then the variance of the encoder 
    $$
    q_{\mathbf{\phi},\mathcal{D}}(\boldsymbol{x},\boldsymbol{z}) = q_{\boldsymbol{\phi}}(\mathbf{z}\mid\mathbf{x})p_{\mathcal{D}}(\mathbf{x})
    $$
    where $p_{\mathcal{D}}(\mathbf{x})$ represents the true underlaying distribuion of data (however only represented by the samples).
    This is due to the direction of the KL divergence: The generative model is only slightly penalized when putting probability mass on values 
    of $(\boldsymbol{x},\boldsymbol{z})$ with no support under $q_{\mathbf{\phi},\mathcal{D}}$\cite{intro-vae-2019}. This corresponds to forward KL
    divergence visualized in (a) in \ref{fig:forward-reverse}.

    \item \textbf{Posterior collapse}: It has been observed, that part of the latent variable can collapsese during the training
    and become inactive \cite{lvae-2016,iaf-2016,bowmann-2016}, i.e. the optimization gets stuck in an undesirable stable
    equilibrium. This is due to the fact that objective $\mathrm{ELBO}$ is composed of the reconstruction error
    and the variational regularization term (see eq. \ref{eq:elbo_terms}). The reconustriction error is initially relatively week and 
    the variational term forces the approximate posterior towards its prior. This can result in many latent units collapsing before they 
    can learn a useful representation. 
    The \cite{lvae-2016} alleviate the problem by scheduling the optimization objective such that the variational regularization term is
    forced to 0 at the beginining of the optimization and then slowly linear ramped up to its value through the optimization. The \cite{iaf-2016} 
    proposes the method of \textit{free bits}, which ensures that on average, a certain minimum information is encoded per (group of) 
    latent variable. 
    \item \textbf{Disentangling latent factors}: Vannila VAEs often learn a tangled representation of the latent space, making it
    difficult to interpret or control individual factors in the data. This can limit their usefulness in applications such as 
    image editing or style transfer.
\end{itemize}

\section{Hiearchical VAE}
A hierarchical variational autoencoder (HVAE) is a generalization of the VAE that introduces multiple layers of latent variables.
The first proposal to address the limitations of the vanilla VAE was the use of auxiliary latent variables, as introduced in \cite{aux-var-2016},
which were followed up by truly hiearchical structure in \cite{lvae-2016} and \cite{iaf-2016}. Standard vanilla VAE contains only one layer
of latent variables (fig. \ref{fig:vanilla-vae}) in contrast to HVAE, such as markov HVAE, where each latent variable depends only on previous one
(fig. \ref{fig:mhvae}). The HVAE can be represented as (almost) any directed acyclic graph and can be represented as DLVM or Bayesian Network with 
following joint and posterior distribuions:
\begin{align}
    p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z}_{1:T})&=p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z}_1) \prod_{t=1}^{T-1}\bigl[p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{>t}) \bigr]  p_{\boldsymbol{\theta}}(\boldsymbol{z}_{T}) \label{eq:hvae_prior} \\
    q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x}) &= q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1}\mid\boldsymbol{x}) \prod_{t=2}^{T}  q_{\boldsymbol{\phi}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{<t}) \label{eq:hvae_posterior}
\end{align}
where $\boldsymbol{z}_{<t} = \boldsymbol{z}_{1:t-1}$ represents the sequence of all latent variables $\boldsymbol{z}_1$ up to $\boldsymbol{z}_t$. 
We can derive the ELBO objective for the HVAE: 
\begin{align}
    \log p_{\boldsymbol{\theta}}(\boldsymbol{x}) &= \log \int_{\boldsymbol{z}_{1:T}} p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z}_{1:T}) \d \boldsymbol{z}_{1:T} \notag\\
    &= \log \int_{\boldsymbol{z}_{1:T}} \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z}_{1:T})q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid \boldsymbol{x})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})} d\boldsymbol{z}_{1:T} \notag \\
    &= \log \EX_{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})} \left[\frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z}_{1:T})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})}\right] \notag\\
    &\geq \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})}\left[\log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{x}, \boldsymbol{z}_{1:T})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})}\right]  = \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})) \label{eq:hvae_elbo}
\end{align}
We can plug the joint and posterior distribuions from eq. \ref{eq:hvae_prior} and  eq. \ref{eq:hvae_posterior} into the ELBO term (eq. \ref{eq:hvae_elbo}) and obtain:

\begin{align}
    \mathrm{ELBO}(q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x}))  &=
    \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})}\left[\log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{z}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z}_1)\prod_{t=1}^{T-1}p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{>t})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}_1\mid\boldsymbol{x})\prod_{t=2}^{T}q_{\boldsymbol{\phi}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{<t})}\right]
\end{align}

\begin{figure}[]
    \centering
    \begin{subfigure}[b]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{vannila_vae.png}
        \caption[]{Vanilla VAE}
        \label{fig:vanilla-vae}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.63\textwidth}
        \centering
        \includegraphics[width=\textwidth]{hvae.png}
        \caption[]{markov HVAE with $T$ layers, each varible $\boldsymbol{z}_i$ depends only on previous $\boldsymbol{z}_{i+1}$, creating a markov chain.}
        \label{fig:mhvae}
    \end{subfigure}
    \caption[VAE vs HVAE architecture]{Visualization of vanilla VAE and markov hiearchical VAE (MHVAE) architecture. Source \cite{diffusion-models-blog-2018}}
    \label{fig:vae-architecture}
\end{figure}


\subsection{Markov HVAE}
The special case of the HVAE is markov HVAE, where each varible $\boldsymbol{z}_i$ depends only on previous variable $\boldsymbol{z}_{i+1}$. This can be
seen as a multiple VAEs stacked on top of each other as represented in fig \ref{fig:mhvae}. The joint and posterior distribuion of HVAE is then
\begin{align*}
    p_{\boldsymbol{\theta}}(\boldsymbol{x},\boldsymbol{z}_{1:T})&=p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z}_1)\prod_{t=1}^{T-1} \bigl[   p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{t+1}) \bigr]  p_{\boldsymbol{\theta}}(\boldsymbol{z}_{T}) \\
    q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x}) &= q_{\boldsymbol{\phi}}(\boldsymbol{z}_1\mid\boldsymbol{x})\prod_{t=2}^{T}q_{\boldsymbol{\phi}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{t-1}) 
\end{align*}
with objective $\mathrm{ELBO}(q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x}))$:
\begin{align*}
    \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}_{1:T}\mid\boldsymbol{x})}\left[\log \frac{p_{\boldsymbol{\theta}}(\boldsymbol{z}_T)p_{\boldsymbol{\theta}}(\boldsymbol{x}\mid\boldsymbol{z}_1)\prod_{t=1}^{T-1}p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{t+1})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}_1\mid\boldsymbol{x})\prod_{t=2}^{T}q_{\boldsymbol{\phi}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{t-1})}\right]
\end{align*}
While MHVAEs can express complex distributions and overcome the issue of blurriness, optimizing them for deep hierarchies can be challenging because of their multiple conditional stochastic layers.
This was partial overcomed with ladder VAE, which proposes different ordering of the variational encoder to improve the learning. 
\subsection{Ladder VAE}
The standard HVAEs (used in \cite{vae-original-2013,iaf-2016,renedze-backprop-2014}), which use a bottom-up encoder and top-down decoder, are
difficult to train due to the lack of interaction between the encoder and decoder during inference, as shown in (a) of fig. \ref{fig:lvae}. 
However, the recently proposed ladder VAE (LVAE) by Sønderby et al. \cite{lvae-2016} addresses this issue by introducing a new inference model
that leverages top-down dependencies, similar to the generative model, as depicted in (b) of fig. \ref{fig:lvae}. This approximate posterior 
distribution merges information from a bottom-up approximate likelihood and top-down prior information. By sharing information and parameters 
with the generative model, the inference model gains access to the current state of the generative model at each layer. The top-down pass then
recursively corrects the generative distribution with a data-dependent approximate log-likelihood.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.55\textwidth]{lvae.png}
    \caption[ladder VAE]{Ilustration of bottom-up and top-down approaches of encoder (inference model) and decoder (generative model). In (a), we can see the standard HVAE and in (b) the newly proposed LVAE.
    Source \cite{lvae-2016}}
    \label{fig:lvae}
\end{figure}
This inference model can be formaly expressed as:
\begin{equation}
q_{\boldsymbol{\phi},\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{>t}, \boldsymbol{x}) \sim p_{\boldsymbol{\theta}}(\boldsymbol{z}_{t}\mid\boldsymbol{z}_{>t})\tilde{q}_{\boldsymbol{\phi}}(\boldsymbol{z}_i \mid \boldsymbol{x}) \label{eq:lvae_encoder}
\end{equation}
where $\tilde{q}_{\boldsymbol{\phi}}(\boldsymbol{z}_i \mid \boldsymbol{x})$ represents the probability distribution from given family $\mathcal{Q}$ parametrized by (deterministic) encoder:
\begin{align*}
\boldsymbol{d}_i &= \mathrm{NeuralNet}_{\boldsymbol{\phi}} (\boldsymbol{d}_{i-1}) , \, \boldsymbol{d}_0 = \boldsymbol{x} \\
\tilde{q}_{\boldsymbol{\phi}}(\boldsymbol{z}_i \mid \boldsymbol{x}) &= \tilde{q}_{\boldsymbol{\phi}}(\boldsymbol{z}_i \mid \boldsymbol{d}_i )
\end{align*}
The multiplication in eq. \ref{eq:lvae_encoder} is a choice of authors, but is favorable in case the distribuions are members of exponential family (see \ref{sec:exp_family})












