\section{Variational Autoencoders (VAEs)}
\label{sec:vae}
The \textit{Variational Autoencoder} or VAE for short is generative model that falls into generative modelling (page \pageref{generative-modelling}). 
It is a neural network architecture that is capable of learning a low-dimensional representation of complex high-dimensional data such as 
images, text, or audio. The VAE is a probabilistic model that learns to approximate the true data distribution by using an encoder network to 
map input data into a latent space, and a decoder network to map the latent space back to the original data space. But before we delve into the details of VAE, 
let's explain the term ``Variational Autoencoders'' itself and what it actually represents. The explanation comes in two parts, first we explain 
autoencoders in \ref{subsec:autoencoders} and then variational inference in \ref{subsec:variational-bayes}. The experinced reader can skip those 
introductory parts and go right to \ref{subsec:vaes}.

\subsection{Autoencoders}
\label{subsec:autoencoders}
An autoencoder was first introduced in the 1980s by Hinton \cite{autoencoders-1986}. However, it was not until the advent of deep learning 
and the availability of large amounts of data and computational resources in the 2000s and 2010s that autoencoders became widely used and 
achieved state-of-the-art results in a variety of task \cite{dim-reduction-ae-2006}. An autoencoder is a neural network designed to learn 
identity mapping in unsupervised manner to reconstruct the original input while compressing the information in the ``bottleneck'' layer 
to obtain a compressed representation \ref{fig:autoencoder}. Through this, we obtain an effecient dimensionality reduction: The low-dimensional 
latent representation can be used as an embedding vector in various application, such as search or data compression \cite{ae-blog-2018}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{autoencoder.png}
    \caption{Ilustration of autonecoder network with two networks: \textit{Encoder} and \textit{Decoder}, each parametrized by learnable parameters.
    Source \cite{ae-blog-2018}}
    \label{fig:autoencoder}
\end{figure}

\subsubsection*{Pluto alegory}
The concept of latent variables in generative modeling can be explained using Plato's Allegory of the Cave \cite{pluto-alegory}. 
In the allegory, people are confined to a cave and can only see two-dimensional shadows of three-dimensional objects projected onto a wall. 
Similarly, the objects we observe in the world may be generated by higher-level abstract concepts that we can never directly observe. 
These abstract concepts may represent properties such as color, size, and shape. Even though we never see and can not fully comprehend these 
higher-level concepts in all details, we can still reason and draw inferences about them through their manifestation in our lifes. In similar
way, we can approximate the latent representations, which encode the observed data \cite{diffusion-models-blog-2018}.  

However, in generative modeling, we typically seek to learn lower-dimensional latent representations rather than higher-dimensional ones. 
This is because attempting to learn a representation of higher dimension than the observation is often difficult without strong priors. 
Learning lower-dimensional latents can also be viewed as a form of compression, which can uncover semantically meaningful structure describing
observations.

The autoencoder's architecture is composed of two networks:
\begin{itemize}
    \item \textit{Encoder} network, which takes (high-dimensional) input $x$ and maps it into low-dimensional latent code $z$. We denote it as a
    function $g(\cdot)$ parametrized by $\phi$. It's goal is to provide dimensionality reduction, just like 
    any other approaches such as principle component analysis (PCA) or t-SNE. 
    \item \textit{Decoder} network, which takes the code $z$ and recovers the data $x$. We donte it as an function $f(\cdot)$ parametrized by $\theta$.
\end{itemize}
The parameters $\theta$ and $\phi$ are learned simultaneously using stochastic gradient descent (SGD). We encode the input, decode it, and
compute the mean squared error (MSE) loss for each feature in the batch to train the model. By minimizing this loss, we encourage the model
to produce output that is as close as possible to the input.
\begin{align*}
    x^\prime &= f_\theta(g_\phi(x)) = f_\theta(z) \\
    \mathcal{L}_{\text{MSE}} &= \frac{1}{n} \sum_{i=1}^{n} (x^{\prime}_i - x_i)^2
\end{align*}
Vanilla autoencoders can be prone to overfitting if their capacity is too high relative to the size of the dataset. To improve their
robustness, Vincent et \textit{al.} proposed a \textit{denoising} autoencoder in 2008 \cite{denoising-ae-2008}. This approach involves adding
random noise to the input and then training the model to reconstruct the original signal. By forcing the model to learn the underlying
structure of the data, rather than just memorizing the training examples, denoising autoencoders can achieve better generalization 
performance. The idea of adding noise is today known as dropout technique \cite{dropout-2014}. 

Since then, other architectures have been proposed to improve robustness and prevent overfitting. These include sparse autoencoders
\cite{sparse-ae-2011}, k-sparse autoencoders \cite{ksparse-ae-2014}, and contractive autoencoders \cite{contractive-ae-2011}. The novel 
approach was then defined in 2013 by Kingma and Welling \cite{vae-original-2022} and VAEs were introduced. The key idea was to assume that 
latent space is not deterministic, but stochastic with some distribution $p(z)$ over it. The goal of the VAE is to model this 
distribution by variational bayes. 

\subsection{Variational Calculus and Variational Bayes}
\label{subsec:variational-bayes}
The idea of autoencoder was described above, but what does the ``\textit{variational}'' in ``variational autoencoder'' stands for? 
To start with, we shall introduce the idea of \textit{variational calculus}. Then we will introduce it in the field of 
\textit{Bayesian statistics}, where it is used as a tool to overcome issuse with complex and non-trivial distributions, which we need to model.  
\subsubsection*{Variational calculus}
The variational calculus is a broad field of mathematical analysis that shares many similarities with the more familiar continuous optimization
and differential calculus techniques. In classical physics, it is also known as "Hamilton's principle" and plays a critical role in Lagrangian
mechanics \cite{lagrangian-mechanics-1998,kulhanek-2016}. The concepts of variational calculus are also extensively used in continuous optimal
control theory \cite{optimal-control-2004} and many other fields.  In this context, we will introduce the concept of variational calculus 
indirectly by defining the problem of optimization, highlighting its similarities to differential calculus.

A real-valued function $f$ defined on a domain $X$ has a global (or absolute) minimum point (minimizer) at $x^\star$, 
if $f(x^\star) \leq f(x)$ for all $x \in X$. The value of the function at a minimum point is called the \textit{minimum value} of 
the function, denoted 
$$
\min_{x \in X}(f(x)).
$$ 
If the domain $X$ is a metric space, then $f$ is said to have a \textit{local minimum point} at $x^\star$, 
if $f(x^\star) \leq f(x)$ for all $x \in X$ within distance $\epsilon$ of $x^\star$.
The set of all \textit{minimum points} is denoted as
$$
\argmin_{x\in X} = \big\{x\in X | f(x) \leq f(x^\prime)\, \forall x^\prime \in X \big\}.
$$ 
In both the global and local cases, the concept of a strict extremum can be defined: $x^\star$ is a \textit{strict local maximum point} if 
there exists some $\epsilon > 0$ such that, for all $x \in X$ within distance $\epsilon$ of $x^\star$ with $x \neq x^\star$, we have 
$f(x^\star) < f(x)$. Note that a point is a strict global minimum point if and only if it is the unique global minimum point. 

The goal of the mathematical optimization (also known as mathematical programming) is to find a minimum (or maximum) of a real valued function 
$f: X^\prime \to \mathbb{R}$ on a subset $X \subset X^\prime$ \cite{werner-opt-2022}. This is very general formulation as the $X$ can be arbitrary.
In case the $X^\prime = \mathbb{R}^n$, we talk about \textit{continuous optimization} and we use the differential calculus to obtain the solution.
If the $X^\prime$ is countable, i.e. there exists an injective function from $X^\prime$ to the set of natural numbers:
$$
X^\prime  \prec \mathbb{N} = \{1,2,3,...\},
$$
we talk about \textit{combinatorical optimization}.
Lastly, if the $X^\prime$ contains functions itself, e.g. all continous real-valued functions on closed interval $[a,b]$:
$$
X^\prime \subset C(a,b),
$$
we talk about \textit{calculus of variations}. 

In variational calculus, we call the small change in the function $x \in X^\prime$ as \textit{variation} and denote it $\delta x$
(compare it with  \textit{differential} of number $\d x$ from differential calculus). The objective function $f$ is called \textit{functional} as 
it maps functions to real numbers $\mathbb{R}$. Functionals are often expressed as definite integrals involving functions and their derivatives.
Even though it is not important for explaining the \textit{variational bayes}, we will mention Euler-Lagrange equations, which plays key role in 
variational calculus. They are system of second-order ordinary differential equations\cite{intro-variational-calc-2003}.They provide the 
stationary points (candidates for extrema) of the given functional $f$. Formaly, let 
\begin{equation*}
    J(f) = \int_a^b L(x,f(x),f^\prime(x)) \d x 
\end{equation*}
be the functional to be minimized. We are looking for a continously differentiable function\cite{smooth-functions-2023} $f \in C^1([a,b])$, which satisfies the boundary conditions $f(a) = A$ and $f(b) = B$.
We also assume that $L$ is twice continously differentiable. Function $f$ is a stationary point of $J$ if and only if
\begin{equation*}
    \frac{\partial L}{\partial f} - \frac{\d}{\d x} \frac{\partial L}{\partial f^\prime} = 0 
\end{equation*}
The derivation of Euler-Lagrange equations is straightforward and is shown in many introductory materials, such as \cite{hurak-2021} or \cite{kulhanek-2016}.

\subsubsection*{Variational Bayes}
Now we know what \textit{variations} are, but how do we apply them in the context of bayesian inference? Let's assume we have a probabilistic
graphical model (PGM)\cite{graphical-models-2023} with some hidden (or unobserved) nodes $H$ and some observed nodes (evidence) $E$. The goal 
of bayesian inference is to compute posterior probability $p(H|E)$:
$$
p(H|E) = \frac{p(H,E)}{p(E)} = \frac{p(E|H) p(H)}{p(E)}
$$ 
where $p(E)$ is marginal density of the evidence:
$$
p(E) = \SumInt_{H} p(H,E)\, \d H
$$
Computing the evidence is for the most of the models intractable due to the integral or higher number of hidden variables (even if they were from
categorical distribuion). The intractability of $p(E)$ is related to the intractability of the
posterior distribution $p(H|E)$. Note that the joint distribution $p(H, E)$ is efficient to compute, and that the densities are related by Bayes formula.
Since $p(H, E)$ is tractable to compute, a tractable marginal likelihood $p(E)$ leads to a tractable posterior $p(H|E)$, and vice versa \cite{intro-vae-2019,vb-intro-1999}.
 
Variational Bayes (VB) is a technique for approximating complex probability distributions by simpler ones and was introduced by Jordan et al. in 1999 \cite{vb-intro-1999}. 
VB provides a way to approximate the posterior distribution by a simpler one that belongs to a tractable family of distributions, and to do so
by minimizing the Kullback-Leibler (KL) divergence between the true posterior and the approximate one. The KL divergence is a functional with respect 
to approximate tractable distribution $q(H)$ since the true posterior $p(H|E)$ is given, hence the connection with \textit{variational calculus}.  
The KL divergence is a measure of the dissimilarity between two probability distributions:
\begin{align*}
q^\star(H) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(H) || p(H | E)) \\
\mathrm{KL}(q(H) || p(H | E)) &= \SumInt q(H) \log \big[ \frac{q(H)}{p(H|E)} \big]\,\d H,
\end{align*}
where $\mathcal{Q}$ is the family of tractable distributions, and $\mathrm{KL}$ is the KL divergence. It should be noted that there are other 
non-optimization based methods to do such approximate inference, such as MCMC \cite{wiki-mcmc-2023}. 

As the KL divergence is not symmetrical, one could ask why we have defined the optimization task as the reverse KL-divergence and not the other way
around, i.e. forward KL divergence $\mathrm{KL}(p(H | E) | q(H))$. Both cases are illustrated in Figure \ref{fig:forward-reverse}. The reverse 
KL~divergence minimization results in $q$ \textit{under-estimating} $p$, which can be perceived as a safe choice. This choice ensures that sampling
from found $q$ provides values which are plausible under original $p$. For a thorough explanation, we refer to the literature 
\cite{another-vb-intro-2021}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fwd-rvrs-kl.png}
    \caption{Ilustration of forward vs reverse KL-divergence on a bimodal distribution. The blue and the red contours represent the target $p$ 
    and the unimodal approximation $q$, respectively.In (a), the forward KL-divergence minimization is visulized with $q$ trying to cover up $p$.
    The (b) and (c) shows the reverse KL-divergence where $q$ locks on to one of the two modes. Source \cite{another-vb-intro-2021}}
    \label{fig:forward-reverse}
\end{figure}

We will now relabel our variables to follow
the standard notation used in deep learning literatue, where hidden variables $H$ are known as latent $\mathbf{z}$ and observed $E$ as features $\mathbf{x}$. 
In this setting, we want to optimize:
\begin{align}
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) \\
    \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) &= \SumInt_\mathbf{z} q(\mathbf{z}) \log \big[ \frac{q(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \big]\,\d \mathbf{z} \\
    &= \SumInt_\mathbf{z} \big[q(\mathbf{z}) \log q(\mathbf{z})\big] \d \mathbf{z} - \SumInt_\mathbf{z}  \big[q(\mathbf{z}) \log p(\mathbf{z}|\mathbf{x})\big] \d \mathbf{z} \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{z}|\mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \Big[\log \big( \frac{p(\mathbf{x},\mathbf{z})}{\mathbf{x}}\big) \Big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \EX_q \big[\log p(\mathbf{x})\big] \\
    &= \EX_q \big[\log q(\mathbf{z})\big] - \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] + \log p(\mathbf{x})
\end{align}
where the $\EX_q \big[\log p(\mathbf{x})\big] = \log p(\mathbf{x})$ because the $p(\mathbf{x})$ does not depend on $q(\mathbf{x})$. 
We can not directly optimize the KL divergence since the evidence $p(\mathbf{x})$ is intractable, however it is constant (for given datset).
If we rearange the last equation, we obtain 
\begin{align}
\log p(\mathbf{x}) - \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) &= \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \\
    &= \mathrm{ELBO}(q)
\end{align}
where the left hand side (LHS) of the equation is called evidence lower bound (ELBO), since it is truly a lower bound on the logarithm of evidence $p(\mathbf{x})$. This is clear to see 
as the KL divergnce is always positive. As the $\log p(\mathbf{x})$ is constant, maximizing the RHS is equal to minimzing the KL-divergence:
\begin{align*} 
    q^\star(\mathbf{z}) &= \argmin_{q \in \mathcal{Q}} \mathrm{KL}(q(\mathbf{z}) || p(\mathbf{z} | \mathbf{x})) \\
                        &= \argmax_{q \in \mathcal{Q}} \mathrm{ELBO}(q) \\
                        &= \argmax_{q \in \mathcal{Q}} \bigl[ \EX_q \big[\log p(\mathbf{x},\mathbf{z})\big] - \EX_q \big[\log q(\mathbf{z})\big] \bigr]  \\
\end{align*}
\subsection{VAE}
\label{subsec:vaes}

