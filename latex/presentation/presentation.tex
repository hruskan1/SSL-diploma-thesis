\documentclass[11pt]{beamer}

\def\cl#1{\mathcal{#1}}
\def\bb#1{\mathbb{#1}}

\def\_#1{\protect\mathchoice
    {\mbox{\boldmath $\displaystyle\bf#1$}}
    {\mbox{\boldmath $\textstyle\bf#1$}}
    {\mbox{\boldmath $\scriptstyle\bf#1$}}
    {\mbox{\boldmath $\scriptscriptstyle\bf#1$}}}

\DeclareMathOperator{\EX}{\mathbb{E}}

\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\arctg}{\mathop{\rm arctg}}
\newcommand{\tg}{\mathop{\rm tg}}
\newcommand{\aff}{\mathop{\rm aff}}
\newcommand{\conv}{\mathop{\rm conv}}
\newcommand{\rank}{\mathop{\rm rank}}
\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\sgn}{\mathop{\rm sgn}}
\newcommand{\Null}{\mathop{\rm null}}
\newcommand{\Rng}{\mathop{\rm rng}}
\newcommand{\dist}{\mathop{\rm dist}}
\renewcommand{\d}[1]{\mbox{\rm d}#1}
\newcommand{\softmax}{\mathop{\rm softmax}}

\usepackage{amsmath}
\usepackage{tabularx,booktabs}
\usepackage{makecell}

% clear the predefined navigation symbols
% \beamertemplatenavigationsymbolsempty
% \setbeamertemplate{footline}{\hfill \insertframenumber\,/\,\insertpresentationendpage}

% print only the frame number in the footline


\title{Semi-Supervised Learning \\for Spatio-Temporal Segmentation of Satellite Images}
\author{Antonín Hruška}
\date{\today}

\begin{document}

\maketitle

\begin{frame}
    \frametitle{Motivation}
    ESA project\footnote{At https://eo4society.esa.int/projects/spatiotemporal-sen2vhr/} in collaboration with Mapradix s.r.o.

    Enhanced Spatiotemporal Land Change Monitoring Based on Sentinel-2 Time Series and VHR Images

    \begin{figure}[b]
        \centering
        \includegraphics[width=0.7\linewidth]{spatio_temporal_segmenting.png}
    \end{figure}

\end{frame}


\begin{frame}
    \frametitle{Remote Sensing and its issues}
    \textbf{Sentinel-2 imagery}
    \begin{itemize}
        \item 13 spectral bands
        \item period of 10 days
        \item different spatial resolution (10/20/60 $m^2$)
    \end{itemize} 
    \textbf{Issues}
    \begin{itemize}
        \item Partial Oclusions: Cloud cover or snow cover (seasonal dependence)
        \item Different viewing angle
        \item Atmospheric calibration etc. 
        \item Annotation process is time demanging and requires high expertise.
    \end{itemize}

    Semi Supervised Learning can reduce the need for large number of annotations.
    Generative models can impaint the ocluded regions. 
\end{frame}


\begin{frame}
    \frametitle{Semi Supervised Learning}
    $$
    \mathcal{D} = \mathcal{X}\,\cup\,\mathcal{U}:\, \mathcal{X} = \{(\mathbf{x}_1,y_1),\dots, (\mathbf{x}_l,y_l)\},\quad \mathcal{U} = \{(\mathbf{x}_{l+1}),\dots,(\mathbf{x}_{u})\},
    $$
    where $x_i$ are features and $y_i$ are labels. 
    
    Assumptions from unsupervised learning: 
    \begin{itemize}
        \item Smothness: $x_1$ close to $x_2$ in high density region imples $y_1$ close to $y_2$
        \item Cluster: Points in one cluster are likely to be of the same class
        \item Manifold: The data lie along low-dimensional latent manifolds inside that high-dimensional space.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{MixMatch}
    \textbf{Discriminative model} based on empirical risk minimization
    {\footnotesize
    \begin{itemize}
        \item Data Augmentation
        \item Pseudo labeling
        \item Sharpening
        \item MixUp
    \end{itemize}
    \begin{equation*}
        \begin{gathered}
            \mathcal{X}^\prime,\mathcal{U}^\prime = \text{MixMatch}(\mathcal{X}, \mathcal{U}, T, K, \alpha)\\
            \mathcal{L}_{\mathcal{X}} = \frac{1}{|\mathcal{X}^\prime|}\sum_{x^\prime,p^\prime \in \mathcal{X}^\prime} H(p^\prime,f_\theta(x^\prime)), \quad
            \mathcal{L}_{\mathcal{U}} = \frac{1}{L|\mathcal{U}^\prime|}\sum_{u^\prime,q^\prime \in \mathcal{U}^\prime} ||q^\prime - f_\theta(u^\prime)||_2^2 \\
            \mathcal{L} = \mathcal{L}_{\mathcal{X}} + \lambda_\mathcal{U} \mathcal{L}_{\mathcal{U}}
        \end{gathered}
    \end{equation*}
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\linewidth]{mixmatch_label_guessing.png}
        \caption{Mixmatch up to the point of sharpening. Source: MixMatch}
    \end{figure}}
    {\footnotesize \href[]{https://arxiv.org/abs/1905.02249}{\textit{MixMatch: A Holistic Approach to Semi-Supervised Learning}} }
\end{frame}

\begin{frame}
    \frametitle{MixMatch}
    \textbf{After sharpening :}
    \begin{align*}
        \mathcal{X}^\star &= \{ (\tilde{x}_i,p_i)\,|\, i \in \{1,\dots,n\}  \}, \, |\mathcal{X}^\star| = n \\
        \mathcal{U}^\star &= \{ (\tilde{u}_{j,k},q_j)\,|\, j \in \{1,\dots,n\},\,k \in \{1,\dots,K\}  \}, \, |\mathcal{U}^\star| = Kn\\
        \mathcal{W} &= \text{Shuffle}(\text{Concat}(\mathcal{X}^\star,\mathcal{U}^\star))
    \end{align*}
    \textbf{MixUp:}
    \begin{align*}
        \begin{bmatrix} \mathcal{X}^\prime \\ \mathcal{U}^\prime \end{bmatrix} &= \text{MixUp}(\begin{bmatrix} \mathcal{X}^\star \\ \mathcal{U}^\star \end{bmatrix},\mathcal{W}) 
    \end{align*}
    where the MixUp($\cdot$,$\cdot$) corresponds to following operation applied elementwise:
    \begin{align*}
        \lambda^\prime &= \max(\lambda,1-\lambda), \text{ where } \lambda \sim \text{Beta}(\alpha,\alpha) \\
        \begin{bmatrix}x^\prime \\ p^\prime\end{bmatrix} &= \lambda^\prime \begin{bmatrix}
            x_1 \\ p_1
        \end{bmatrix} + (1-\lambda^\prime) \begin{bmatrix}
            x_2 \\ p_2
        \end{bmatrix} \\
    \end{align*}
\end{frame}

\begin{frame}
    \frametitle{VAE}
    \textbf{Generative model} based on ML maximization

    Simple bayesian network with hidden variables $z$:
    $$
    p_{\theta}(x,z) = p_{\theta}(z)p_{\theta}(x\mid z)
    $$
    where $p_{\theta}(z)$ or $p_{\theta}(x\mid z)$ is specified and parametrized by NN.

    The intractability of evidence:
    $$
    p_{\theta}(x) = \int_{z} p_{\theta}(x,z)\, \d z = \frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}
    $$ 
    do not allow us to maximize the log-likelihood directly, instead we introduce the \textit{inference model} $q_{\phi}(z|x)$
    and optimize the ELBO proxy:
    \begin{align*}
        \log p_\theta(x) &= \EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{p_\theta(x,z)q_\phi(z|x)}{q_\phi(z|x)p_\theta(z|x)} \Bigr] \\  
        &=\underbrace{\EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{p_\theta(x,z)}{q_\phi(z|x)} \Bigr]}_{\mathrm{ELBO}} + 
        \underbrace{\EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{q_\phi(z|x)}{p_{\theta}(z|x)} \Bigr]}_{\mathrm{KL}(q_{\phi}(z|x) || p_{\theta}(z|x))}
    \end{align*}
        % 
\end{frame}

\begin{frame}
    \frametitle{VAE}
    \textbf{Learning :} Stochastic approach - unbaiessed estimator and reparametrization trick allow us to use SGD
    \begin{align*}
    \mathcal{L} = \EX_{q_{\phi}(z|x)} \Bigl[ \log {p_\theta(x,z)} \Bigr] - \EX_{q_{\phi}(z|x)} \Bigl[ \log {q_\phi(z|x)} \Bigr] 
    \end{align*}
    \textbf{Issues and limitations:} 
    \begin{itemize}
        \item Parametrization trick requires continouous latent variable
        \item Posterior collapse: Optimization can end in an undesirable stable equilibrium: $q_{\phi}(z|x) \approx p_{\theta}(z)$
        \item Bluriness of images: The ELBO enforces the consistent encoder and decoder pairs. This disallow the model to learn 
        the complex data distribution. Solved by more flexible model (such as hiearchical VAE).
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Symmetric equillibrium leaning in VAE}
    New objectives are 
    \begin{align*}
        \mathcal{L}_{p}(\theta,\phi) =& \EX_{\pi (x,z)}\bigl[\log p_{\theta}(x,z)\bigr]+
                        \EX_{\pi(z)}\bigl[\log p_{\theta}(z)\bigr] +\\
                        &\EX_{\pi(x)}\EX_{q_{\phi}(z|x)}\bigl[\log p_{\theta}(x,z)\bigr]\\
        \mathcal{L}_{q}(\theta,\phi) =& \EX_{\pi(x,z)}\bigl[\log q_{\phi}(z | x)\bigr] + 
        \EX_{\pi(z)}\EX_{p_{\theta}(x | z)} \bigl[\log q_{\phi}( z | x)\bigr]
    \end{align*}
    for semi-supervised training instead of ELBO:
    $$
    \EX_{\pi(x)} \bigl[ 
    \log p_{\theta}(x) - \mathrm{KL} (q_{\phi}(z | x) || p_{\theta}(z | x)) \bigr]
    $$
    The optimization does not require the reparametrization trick and allows for broader families of distributions,
    such as exponential family. 
    \textit{Symmetric Equilibrium Learning of VAE} [submitted to NeurIPS 23]
\end{frame}

\begin{frame}
\frametitle{Methods}
\begin{itemize}
    \item CityScape dataset (simple 2D segmentation)
    \item U-net backbone
    \item plain accuracy and IoU metric
    \item hiearchical VAE for symmetric equilibrium learning 
    \begin{align*}
        \mathbf{z} = (\mathbf{z}_0, \mathbf{z}_1, \dots, \mathbf{z}_m), \quad \mathbf{z}_0 = (\mathbf{s}, \mathbf{l}),\, \mathbf{z}_m = \mathbf{x}
    \end{align*}
    with blockwise optimization:
    \begin{align*}
        \mathcal{L}_{p}(\_\theta,\_\phi) &= 
            \EX_{\pi(\_x,\_s)}\EX_{q_{\_\theta,\_\phi}(\_z_{>0},\_l \mid \_x, \_s)}\bigl[\log p_{\_\theta}(\_x,\_z)\bigr]\\
           &+\EX_{\pi(\_x)}\EX_{q_{\_\theta,\_\phi}(\_z \mid \_x)} \bigl[\log p_{\_\theta}(\_x,\_z)\bigr] \\
        \mathcal{L}_{q}(\_\theta,\_\phi) &=
            \EX_{\pi(\_x,\_s)}\bigl[\log q_{\_\phi}(\_s \mid \_x) \bigr] \\
           &+\EX_{\pi(\_s)}\EX_{p(\_l)}\EX_{p_{\_\theta}(\_x,\_z_{>0} \mid \_z_{0})} \bigl[\log q_{\_\theta,\_\phi}(\_z | \_x)\bigr] 
    \end{align*}
    $\pi(\_x,\_s)$ represents the underlying distribution with marginals $\pi(\_x)$ and $\pi(\_s)$. 
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Architecture of the HVAE}
    \begin{figure}[t]
        \centering
        \includegraphics[width=\textwidth]{Unet.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Mixmatch results (tables)}
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        Mixmatch & $84.53$ & $90.76$ & $93.42$ & $94.32$ & $94.84$ \\
        \hline
        Supervised & $76.85$ & $87.59$ & $93.50$ & $94.71$ & $95.58$ \\
        \hline
        \end{tabular}
        \caption{Mixmatch accuracy rate (\%) on CityScape dataset}
    \end{table}
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        Mixmatch & $48.10$ & $61.98$ & $69.24$ & $71.97$ & $73.08$ \\
        \hline
        Supervised & $41.78$ & $54.38$ & $68.25$ & $71.93$ & $73.84$ \\
        \hline
        \end{tabular}
        \caption{Mixmatch average IoU (\%) on CityScape dataset}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{Mixmatch results (images)}
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{mix10_vizualization.png}
        \caption{Models were trained on \textbf{ten} images. The first two rows contain the image and its ground truth segmentation. The MixMatch predictions are shown in the third row, 
        while the supervised baseline predictions are in the fourth.\newline
        The colors used in the segmentation are as follows: flat (purple), human (red), vehicle (dark blue), construction (dark grey), 
        object (light gray), nature (green), sky (light blue), and void (black). The predictions on the "void" class are not penalized nor evaluated.}
    \end{figure}
\end{frame}
\begin{frame}
    \frametitle{Mixmatch results (images)}
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{mixall_vizualization.png}
        \caption{Models were trained on \textbf{all} images. The first two rows contain the image and its ground truth segmentation. The MixMatch predictions are shown in the third row, 
        while the supervised baseline predictions are in the fourth.\newline
        The colors used in the segmentation are as follows: flat (purple), human (red), vehicle (dark blue), construction (dark grey), 
        object (light gray), nature (green), sky (light blue), and void (black). The predictions on the "void" class are not penalized nor evaluated.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Symmetric equilibrium training results}    
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        HVAE & $74.01$ & $86.18$ & $90.23$ & $92.63$ & $93.51$ \\
        \hline
        Supervised & $74.82$ & $87.73$ & $90.81$ &$44.05^\ast$ &  $43.37^\ast$  \\
        \hline
        \end{tabular}
        \caption[HVAE plain accuracy on CityScape]{HVAE accuracy rate (\%), $^\ast$ (learning has failed) }
        \label{tab:hvae-cityscapes-acc}
    \end{table}
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=\textwidth]{shbvae_cs_vizualization.png}
        \caption[Symmetric learning HVAE CityScape results]{The first row contains the original image. The second row is filled with the reconstructed
        images from encoding the original image into $\mathbf{z}_0$ and decoding. The third and fourth rows contain model and ground truth segmentation, respectively.}
        \label{fig:hvae-cs}
    \end{figure}
\end{frame}

    
\end{document}
