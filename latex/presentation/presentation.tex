\documentclass[aspectratio=169,11pt]{beamer}
\def\cl#1{\mathcal{#1}}
\def\bb#1{\mathbb{#1}}

\def\_#1{\protect\mathchoice
    {\mbox{\boldmath $\displaystyle\bf#1$}}
    {\mbox{\boldmath $\textstyle\bf#1$}}
    {\mbox{\boldmath $\scriptstyle\bf#1$}}
    {\mbox{\boldmath $\scriptscriptstyle\bf#1$}}}

\DeclareMathOperator{\EX}{\mathbb{E}}

\newcommand{\argmax}{\mathop{\rm argmax}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\arctg}{\mathop{\rm arctg}}
\newcommand{\tg}{\mathop{\rm tg}}
\newcommand{\aff}{\mathop{\rm aff}}
\newcommand{\conv}{\mathop{\rm conv}}
\newcommand{\rank}{\mathop{\rm rank}}
\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\sgn}{\mathop{\rm sgn}}
\newcommand{\Null}{\mathop{\rm null}}
\newcommand{\Rng}{\mathop{\rm rng}}
\newcommand{\dist}{\mathop{\rm dist}}
\renewcommand{\d}[1]{\mbox{\rm d}#1}
\newcommand{\softmax}{\mathop{\rm softmax}}

\usepackage{amsmath}
\usepackage{tabularx,booktabs}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subcaption}

% clear the predefined navigation symbols
\beamertemplatenavigationsymbolsempty
\setbeamerfont{page number in foot}{size=\Large}
\setbeamertemplate{footline}[frame number]

% print only the frame number in the footline


\title{Semi-Supervised Learning \\for Spatio-Temporal Segmentation of Satellite Images}
\author{Antonín Hruška}
\date{\today}

\begin{document}

\maketitle

\begin{frame}
    \frametitle{Motivation}
    ESA project: Enhanced Spatiotemporal Land Change Monitoring Based on Sentinel-2 Time Series and VHR Images

    \textbf{Main motivation:}  Desire to segment the time series of satellite imagery.
    \begin{figure}[b]
        \centering
        \includegraphics[width=0.75\linewidth]{spatio_temporal_segmenting.png}
    \end{figure}  
    
    {\footnotesize\textbf{Challanges:}    
    \begin{itemize}
            \item Only 2\% of the data is annotated. The rest is unlabeled. (5/250) 
            \item Partial occlusion in the data (clouds, snow) = missing measurements 
            \item No validation data 
    \end{itemize}
    \textbf{Proposed solutions:}
    \begin{itemize}
        \item Apply Semi supervised learning 
        \item Evaluate the algorithms on the CityScape dataset instead of satellite images. 
    \end{itemize}}
\end{frame}

\begin{frame}
    \frametitle{Considered SSL methods}
    $$
    \mathcal{D} = \mathcal{X}\,\cup\,\mathcal{U}:\, \mathcal{X} = \{(\mathbf{x}_1,y_1),\dots, (\mathbf{x}_l,y_l)\},\quad \mathcal{U} = \{(\mathbf{x}_{l+1}),\dots,(\mathbf{x}_{u})\},
    $$
    where $x_i$ are features and $y_i$ are labels. 
    \begin{enumerate}
        \item \textbf{Discriminative model} + MixMatch 
        \item \textbf{Generative model} (hiearchical variational autoencoder) + symmetric training
    \end{enumerate}
    
    \textbf{Main contributions:} Implementing and adapting both methods to the task of semantic segmentation 
\end{frame}

\begin{frame}
    \frametitle{MixMatch}
    \textbf{Holistic method} combining the data augmentation, pseudo-labeling, entropy minimization and Mix Up procedure:
{\footnotesize
    \begin{equation*}
        \begin{gathered}
            \mathcal{X}^\prime,\mathcal{U}^\prime = \text{MixMatch}(\mathcal{X}, \mathcal{U}, T, K, \alpha)\\
            \mathcal{L} = \frac{1}{|\mathcal{X}^\prime|}\sum_{x^\prime,p^\prime \in \mathcal{X}^\prime} H(p^\prime,f_\theta(x^\prime))
            + \lambda_\mathcal{U} \frac{1}{L|\mathcal{U}^\prime|}\sum_{u^\prime,q^\prime \in \mathcal{U}^\prime} ||q^\prime - f_\theta(u^\prime)||_2^2 
        \end{gathered}
    \end{equation*}

    \begin{figure}
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centring
        \includegraphics[width=\textwidth]{mixmatch_label_guessing.png}
        \caption{Pseudo labeling}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centring
        \includegraphics[width=\textwidth]{mixup.png}
        \caption{Mix Up}
    \end{subfigure}
    \caption{MixMatch procedure. {\footnotesize \href[]{https://arxiv.org/abs/1905.02249}{\textit{MixMatch: A Holistic Approach to Semi-Supervised Learning}} }}
    \end{figure}    
}

\end{frame}

\begin{frame}
    \frametitle{MixMatch}
    
    \textbf{MixUp after sharpening :}
    {\footnotesize
    \begin{align*}
        \mathcal{X}^\star &= \{ (\tilde{x}_i,p_i)\,|\, i \in \{1,\dots,n\}  \}, \, |\mathcal{X}^\star| = n \\
        \mathcal{U}^\star &= \{ (\tilde{u}_{j,k},q_j)\,|\, j \in \{1,\dots,n\},\,k \in \{1,\dots,K\}  \}, \, |\mathcal{U}^\star| = Kn\\
        \mathcal{W} &= \text{Shuffle}(\text{Concat}(\mathcal{X}^\star,\mathcal{U}^\star))\\
        \begin{bmatrix} \mathcal{X}^\prime \\ \mathcal{U}^\prime \end{bmatrix} &= \text{MixUp}(\begin{bmatrix} \mathcal{X}^\star \\ \mathcal{U}^\star \end{bmatrix},\mathcal{W}) 
    \end{align*}
    where the MixUp($\cdot$,$\cdot$) corresponds to following operation applied elementwise:
    \begin{align*}
        \begin{bmatrix}x^\prime \\ p^\prime\end{bmatrix} &= \lambda^\prime \begin{bmatrix}
            x_1 \\ p_1
        \end{bmatrix} + (1-\lambda^\prime) \begin{bmatrix}
            x_2 \\ p_2
        \end{bmatrix}, \quad \lambda^\prime = \max(\lambda,1-\lambda), \text{ where } \lambda \sim \text{Beta}(\alpha,\alpha)
    \end{align*}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.8\linewidth]{mixup.png}
        \caption{Source: https://hoya012.github.io/blog/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks-Review/}
    \end{figure}
    }
    
\end{frame}

\begin{frame}
    \frametitle{Variational Autoencoders}
    \textbf{Generative model} based on ML maximization

    Simple bayesian network with hidden variables $z$:
    $$
    p_{\theta}(x,z) = p_{\theta}(z)p_{\theta}(x\mid z)
    $$
    where $p_{\theta}(z)$ or $p_{\theta}(x\mid z)$ is specified and parametrized by NN.

    The intractability of evidence:
    $$
    p_{\theta}(x) = \int_{z} p_{\theta}(x,z)\, \d z = \frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}
    $$ 
    does not allow us to maximize the log-likelihood directly, instead we introduce the \textit{inference model} $q_{\phi}(z|x)$
    with amortized inference and optimize the ELBO proxy:
    \begin{align*}
        \log p_\theta(x) &= \EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{p_\theta(x,z)q_\phi(z|x)}{q_\phi(z|x)p_\theta(z|x)} \Bigr] \\  
        &=\underbrace{\EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{p_\theta(x,z)}{q_\phi(z|x)} \Bigr]}_{\mathrm{ELBO}} + 
        \underbrace{\EX_{q_{\phi}(z|x)} \Bigl[ \log \frac{q_\phi(z|x)}{p_{\theta}(z|x)} \Bigr]}_{\mathrm{KL}(q_{\phi}(z|x) || p_{\theta}(z|x))}
    \end{align*}
        % 
\end{frame}

\begin{frame}
    \frametitle{Symmetric equillibrium leaning in VAE}
    New objectives are 
    \begin{align*}
        \mathcal{L}_{p}(\theta,\phi) =& \EX_{\pi (x,z)}\bigl[\log p_{\theta}(x,z)\bigr]+
                        \EX_{\pi(z)}\bigl[\log p_{\theta}(z)\bigr] +\\
                        &\EX_{\pi(x)}\EX_{q_{\phi}(z|x)}\bigl[\log p_{\theta}(x,z)\bigr] \to \max_{\theta} \\
        \mathcal{L}_{q}(\theta,\phi) =& \EX_{\pi(x,z)}\bigl[\log q_{\phi}(z | x)\bigr] +\\ 
        &\EX_{\pi(z)}\EX_{p_{\theta}(x | z)} \bigl[\log q_{\phi}( z | x)\bigr] \to \max_{\phi}
    \end{align*}
    for semi-supervised training instead of ELBO:
    $$
    \EX_{\pi(x)} \bigl[ 
    \log p_{\theta}(x) - \mathrm{KL} (q_{\phi}(z | x) || p_{\theta}(z | x)) \bigr]
    $$
    The $\pi(x,z)$ is underlying unknown true distribution with its marginals $\pi(x)$ and $\pi(z)$.
    \begin{itemize}
    \item does not require the reparametrization trick 
    \item learning possible when both the data and latent distributions are accessible only through sampling.
    \end{itemize}
    \textit{Symmetric Equilibrium Learning of VAE} [submitted to NeurIPS 23]
\end{frame}

\begin{frame}
\frametitle{Methods and Experiments}
\begin{itemize}
    \item CityScape dataset (simple 2D segmentation)
    \item plain accuracy and IoU metric
    \item U-net backbone for both MixMatch and HVAE encoder.
    \item binary hiearchical VAE for symmetric equilibrium learning 
    \begin{align*}
        \mathbf{z} = (\mathbf{z}_0, \mathbf{z}_1, \dots, \mathbf{z}_m), \quad \mathbf{z}_0 = (\mathbf{s}, \mathbf{l}),\, \mathbf{z}_m = \mathbf{x}
    \end{align*}
    where $s$ is categorical distribution to model segmentation, and $l$ is latent code.  The model is optimized blockwise:
    \begin{align*}
        \mathcal{L}_{p}(\_\theta,\_\phi) &= 
            \EX_{\pi(\_x,\_s)}\EX_{q_{\_\theta,\_\phi}(\_z_{>0},\_l \mid \_x, \_s)}\bigl[\log p_{\_\theta}(\_x,\_z)\bigr]\\
           &+\EX_{\pi(\_x)}\EX_{q_{\_\theta,\_\phi}(\_z \mid \_x)} \bigl[\log p_{\_\theta}(\_x,\_z)\bigr] \\
        \mathcal{L}_{q}(\_\theta,\_\phi) &=
            \EX_{\pi(\_x,\_s)}\bigl[\log q_{\_\phi}(\_s \mid \_x) \bigr] \\
           &+\EX_{\pi(\_s)}\EX_{p(\_l)}\EX_{p_{\_\theta}(\_x,\_z_{>0} \mid \_z_{0})} \bigl[\log q_{\_\theta,\_\phi}(\_z | \_x)\bigr] 
    \end{align*}
    $\pi(\_x,\_s)$ represents the underlying distribution with marginals $\pi(\_x)$ and $\pi(\_s)$. 
\end{itemize}
\end{frame}

\begin{frame}
    \footnotesize 
    R: The final architecture used is not explained to the necessary level of detail. 
    A figure and description of the learning process desirable:
    \frametitle{Architecture of the HVAE}
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.7\textwidth]{Unet.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{MixMatch adaptation}
    \textbf{Challanges:}
    \begin{itemize}
        \item Augmentation must be applied to both the image and the segmentation, and it needs to be invertible to compute the average.
        \item Part of the images can be cropped out when applying the inverse transformation. 
    \end{itemize}

    \textbf{Solution:}
    We allow for affine transformation, which are invertible. The process is as follows:
    \begin{itemize}
        \item We apply K (random) augmentations to image.
        \item We classify the augmentation by the network and invert it.
        \item We compute the average over the K augmented views, taking into account only the valid views for each pixel. We then sharpen it.
        \item We apply (the same) augmentations to the pseudo label and assign them to the views. 
        \item We adapt the MixUp procedure to correctly mix cropped images without propagating empty parts.
    \end{itemize}
    

    
\end{frame}

\begin{frame}
    \frametitle{Mixmatch results (tables)}
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        Mixmatch & $84.53$ & $90.76$ & $93.42$ & $94.32$ & $94.84$ \\
        \hline
        Supervised & $76.85$ & $87.59$ & $93.50$ & $94.71$ & $95.58$ \\
        \hline
        \end{tabular}
        \caption{Mixmatch accuracy rate (\%) on CityScape dataset}
    \end{table}
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        Mixmatch & $48.10$ & $61.98$ & $69.24$ & $71.97$ & $73.08$ \\
        \hline
        Supervised & $41.78$ & $54.38$ & $68.25$ & $71.93$ & $73.84$ \\
        \hline
        \end{tabular}
        \caption{Mixmatch average IoU (\%) on CityScape dataset}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{Mixmatch results (images)}
    \begin{figure}[t]
        \centering
        \includegraphics[width=1\textwidth]{mix100_visualization.png}
        \caption{Models were trained on \textbf{100} images. The first two rows contain the image and its ground truth segmentation. The MixMatch predictions are shown in the third row, 
        while the supervised baseline predictions are in the fourth}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Symmetric equilibrium training results}
    {\footnotesize R: There are no “baseline” experiments for symmetric learning. The reviewer proposes possible experiments to conduct.}
    \begin{table}[tbh]
        \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Labels [\#]  & 10 & 100 & 500 & 1000 & All \\
        \hline
        HVAE & $74.01$ & $86.18$ & $90.23$ & $92.63$ & $93.51$ \\
        \hline
        Supervised & $74.82$ & $87.73$ & $90.81$ &$94.18$& $94.93$ \\
        \hline
        \end{tabular}
        \caption[HVAE plain accuracy on CityScape]{HVAE accuracy rate (\%) }
        \label{tab:hvae-cityscapes-acc}
    \end{table}
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.9\textwidth]{shbvae_cs_vizualization.png}
        \caption[Symmetric learning HVAE CityScape results]{The first row contains the original image. The second row is filled with the reconstructed
        images. The third and fourth rows contain model and ground truth segmentation, respectively.}
        \label{fig:hvae-cs}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item The mixmatch can improve the results especially in scenario with low amount of annotated data.
        \item The HVAE in its current state is not capable of generating the images, which could improve the encoder accuracy 
        \item Despite its limitations, the HVAE shows promise as a potential approach worth exploring further.
    
    \end{itemize}
\end{frame}

    
\end{document}
