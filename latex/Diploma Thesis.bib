@article{sym-learning-2023,
  author    = {Dmitrij Schlesinger and
               Oleksandr Shekhovtsov and
               Flach Boris
               },
  title     = {Symmetric Equilibrium Learning of VAE},
  journal   = {NeurIPS},
  year      = {2023},
  intype    = {to appear in}
}

@article{mixmatch-2019,
  author    = {David Berthelot and
               Nicholas Carlini and
               Ian J. Goodfellow and
               Nicolas Papernot and
               Avital Oliver and
               Colin Raffel},
  title     = {MixMatch: {A} Holistic Approach to Semi-Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1905.02249},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.02249},
  eprinttype = {arXiv},
  eprint    = {1905.02249},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-02249.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wide-resnet-2017,
      title={Wide Residual Networks}, 
      author={Sergey Zagoruyko and Nikos Komodakis},
      year={2017},
      eprint={1605.07146},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{remixmatch-2020,
      title={ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring}, 
      author={David Berthelot and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Kihyuk Sohn and Han Zhang and Colin Raffel},
      year={2020},
      eprint={1911.09785},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fixmatch-2020,
      title={FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence}, 
      author={Kihyuk Sohn and David Berthelot and Chun-Liang Li and Zizhao Zhang and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Han Zhang and Colin Raffel},
      year={2020},
      eprint={2001.07685},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cifar10-2009,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}

@article{ssl-overview-2020,
  author    = {Yassine Ouali and
               C{\'{e}}line Hudelot and
               Myriam Tami},
  title     = {An Overview of Deep Semi-Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/2006.05278},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.05278},
  eprinttype = {arXiv},
  eprint    = {2006.05278},
  timestamp = {Fri, 12 Jun 2020 14:02:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-05278.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lvae-2016,
      title={Ladder Variational Autoencoders}, 
      author={Casper Kaae Sønderby and Tapani Raiko and Lars Maaløe and Søren Kaae Sønderby and Ole Winther},
      year={2016},
      eprint={1602.02282},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{unet-2015,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{intro-vae-2019,
	doi = {10.1561/2200000056},
  
	url = {https://doi.org/10.1561%2F2200000056},
  
	year = 2019,
	publisher = {Now Publishers},
  
	volume = {12},
  
	number = {4},
  
	pages = {307--392},
  
	author = {Diederik P. Kingma and Max Welling},
  
	title = {An Introduction to Variational Autoencoders},
  
	journal = {Foundations and Trends{\textregistered} in Machine Learning}
}

@book{ssl-book-2006,
  added-at = {2019-07-22T00:00:00.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/265ac136f8b3a44d77fcf9ec42829296a/dblp},
  editor = {Chapelle, Olivier and Schölkopf, Bernhard and Zien, Alexander},
  ee = {https://doi.org/10.7551/mitpress/9780262033589.001.0001},
  interhash = {90eecf83da2790cac977f375160081fe},
  intrahash = {65ac136f8b3a44d77fcf9ec42829296a},
  isbn = {9780262033589},
  keywords = {dblp},
  publisher = {The MIT Press},
  timestamp = {2019-09-17T12:36:24.000+0200},
  title = {Semi-Supervised Learning},
  url = {http://dblp.uni-trier.de/db/books/collections/CSZ2006.html},
  year = 2006
}

@book{vapnik-quote-2006,
author = {Vapnik, Vladimir and Kotz, S.},
title = {Estimation of Dependences Based on Empirical Data: Empirical Inference Science (Information Science and Statistics)},
year = {2006},
isbn = {0387308652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

# K-means
@inproceedings{k-means-1967,
  title={Classification and analysis of multivariate observations},
  author={MacQueen, J},
  booktitle={5th Berkeley Symp. Math. Statist. Probability},
  pages={281--297},
  year={1967},
  organization={University of California Los Angeles LA USA}
}

@article{k-means-1982,
  title={Least squares quantization in PCM},
  author={Lloyd, Stuart},
  journal={IEEE transactions on information theory},
  volume={28},
  number={2},
  pages={129--137},
  year={1982},
  publisher={IEEE}
}


@inproceedings{entropy-min-2004,
author = {Grandvalet, Yves and Bengio, Y.},
year = {2004},
month = {01},
pages = {},
title = {Semi-supervised Learning by Entropy Minimization},
volume = {17},
journal = {Adv. Neural Inform. Process. Syst.}
}

@inproceedings{how-to-evalute-ssl-2018,
 author = {Oliver, Avital and Odena, Augustus and Raffel, Colin A and Cubuk, Ekin Dogus and Goodfellow, Ian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf},
 volume = {31},
 year = {2018}
}

@techreport{another-survey-2008,
  added-at = {2008-06-16T04:10:48.000+0200},
  author = {Zhu, Xiaojin},
  biburl = {https://www.bibsonomy.org/bibtex/2f30e96810896e4cad560e59e3c24730f/mkroell},
  institution = {Computer Sciences, University of Wisconsin-Madison},
  interhash = {05c1b88e40d74938a36fbd67d02c9bce},
  intrahash = {f30e96810896e4cad560e59e3c24730f},
  keywords = {SemiSupervised classification learning survey},
  number = 1530,
  timestamp = {2008-12-23T14:20:23.000+0100},
  title = {Semi-Supervised Learning Literature Survey},
  url = {http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf},
  year = 2005
}

@misc{mixup-2018,
      title={mixup: Beyond Empirical Risk Minimization}, 
      author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
      year={2018},
      eprint={1710.09412},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{weight-decay-2019,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{psuedo-label-2013,
author = {Lee, Dong-Hyun},
year = {2013},
month = {07},
pages = {},
title = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
journal = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)}
}

@misc{temporal-ensembling-2017,
      title={Temporal Ensembling for Semi-Supervised Learning}, 
      author={Samuli Laine and Timo Aila},
      year={2017},
      eprint={1610.02242},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{mean-teacher-2018,
      title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}, 
      author={Antti Tarvainen and Harri Valpola},
      year={2018},
      eprint={1703.01780},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{regularization-&-pertrubations-2016,
      title={Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning}, 
      author={Mehdi Sajjadi and Mehran Javanmardi and Tolga Tasdizen},
      year={2016},
      eprint={1606.04586},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{autoencoders-1986,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}

@article{dim-reduction-ae-2006,
author = {G. E. Hinton  and R. R. Salakhutdinov },
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}

@inproceedings{denoising-ae-2008,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Y. and Manzagol, Pierre-Antoine},
year = {2008},
month = {01},
pages = {1096-1103},
title = {Extracting and composing robust features with denoising autoencoders},
journal = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390294}
}

@article{dropout-2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
year = {2014},
month = {06},
pages = {1929-1958},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
volume = {15},
journal = {Journal of Machine Learning Research}
}

@misc{sparse-ae-2011,
  author        = {Andrew Ng},
  title         = {CS294A Deep Learning and Unsupervised Feature Learning lecture notes},
  month         = {January},
  year          = {2011},
  publisher={Standford University}
}

@misc{ksparse-ae-2014,
      title={k-Sparse Autoencoders}, 
      author={Alireza Makhzani and Brendan Frey},
      year={2014},
      eprint={1312.5663},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{contractive-ae-2011,
  added-at = {2019-04-03T00:00:00.000+0200},
  author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/28f26a7044285547aa837507603c30b77/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2011},
  editor = {Getoor, Lise and Scheffer, Tobias},
  ee = {https://icml.cc/2011/papers/455_icmlpaper.pdf},
  interhash = {e19f80ada701efa67ab82010144c430e},
  intrahash = {8f26a7044285547aa837507603c30b77},
  keywords = {dblp},
  pages = {833-840},
  publisher = {Omnipress},
  timestamp = {2019-04-04T11:48:16.000+0200},
  title = {Contractive Auto-Encoders: Explicit Invariance During Feature Extraction.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2011.html#RifaiVMGB11},
  year = 2011
}

@article{ae-blog-2018,
  title   = "From Autoencoder to Beta-VAE",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-08-12-vae/"
}

# VAE 

@misc{vae-original-2013,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2013},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{vae-ssl-dgm-2014,
      title={Semi-Supervised Learning with Deep Generative Models}, 
      author={Diederik P. Kingma and Danilo J. Rezende and Shakir Mohamed and Max Welling},
      year={2014},
      eprint={1406.5298},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@manual{vae-introduction-blog-2016,
  title        = {Tutorial - What is a Variational Autoencoder?},
  author       = {Altosaar, Jaan},
  month        = aug,
  year         = 2016,
  doi          = {10.5281/zenodo.4462916},
  url          = {https://doi.org/10.5281/zenodo.4462916}
}

@misc{plato-alegory,
   author = "Wikipedia",
   title = "{Allegory of the cave} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2023",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Allegory\%20of\%20the\%20cave&oldid=1151553170}},
   note = "[Online; accessed 06-May-2023]"
 }

 @article{diffusion-models-blog-2018,
  title   = "Understanding Diffusion Models: A Unified Perspective",
  author  = "Luo, Calvin",
  journal = "calvinyluo.com",
  year    = "2022",
  url     = "https://calvinyluo.com/2022/08/26/diffusion-tutorial.html"
}

# Variational 


@incollection{lagrangian-mechanics-1998,
  title     = "{LAGRANGIAN} {MECHANICS}",
  booktitle = "Analytical Mechanics",
  author    = "Hand, Louis N and Finch, Janet D",
  publisher = "Cambridge University Press",
  pages     = "1--43",
  month     =  nov,
  year      =  1998,
  address   = "Cambridge"
}

@book{optimal-control-2004,
  title     = "Optimal Control Theory",
  author    = "Kirk, Donald E",
  publisher = "Dover Publications",
  series    = "Dover Books on Electrical Engineering",
  month     =  apr,
  year      =  2004,
  address   = "Mineola, NY"
}


@BOOK{intro-variational-calc-2003,
  title     = "An introduction to the calculus of variations",
  author    = "Fox, Charles",
  publisher = "Dover Publications",
  series    = "Dover Books on Mathematics",
  month     =  mar,
  year      =  1987,
  address   = "Mineola, NY"
}




 @article{intro-variational-blog-2019,
  title   = "Introduction to Variational Inference",
  author  = "Mao, Lei",
  journal = "leimao.github.io",
  year    = "2019",
  url     = "https://leimao.github.io/article/Introduction-to-Variational-Inference/"
}

@misc{graphical-models-2023,
   author = "Wikipedia",
   title = "{Graphical model} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2023",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Graphical\%20model&oldid=1131124218}},
   note = "[Online; accessed 07-May-2023]"
 }

 

 @article{vb-intro-1999,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

@misc{wiki-mcmc-2023,
   author = "Wikipedia",
   title = "{Markov chain Monte Carlo} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2023",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Markov\%20chain\%20Monte\%20Carlo&oldid=1148174830}},
   note = "[Online; accessed 07-May-2023]"
 }

 @misc{another-vb-intro-2021,
      title={An Introduction to Variational Inference}, 
      author={Ankush Ganguly and Samuel W. F. Earp},
      year={2021},
      eprint={2108.13083},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{em-as-vb-1998,
  title={A view of the EM algorithm that justifies incremental, sparse, and other variants},
  author={Neal, Radford M and Hinton, Geoffrey E},
  journal={Learning in graphical models},
  pages={355--368},
  year={1998},
  publisher={Springer}
}

@article{em-alg-1977,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 urldate = {2023-05-07},
 volume = {39},
 year = {1977}
}

@inproceedings{amortized-inference-2014,
  title={Amortized inference in probabilistic reasoning},
  author={Gershman, Samuel and Goodman, Noah},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={36},
  number={36},
  year={2014}
}


@InProceedings{renedze-backprop-2014,
  title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author = 	 {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1278--1286},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/rezende14.html},
  abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@inproceedings{iaf-2016,
 author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
 volume = {29},
 year = {2016}
}


@InProceedings{nf-2015,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}


@InProceedings{aux-var-2016,
  title = 	 {Auxiliary Deep Generative Models},
  author = 	 {Maaløe, Lars and Sønderby, Casper Kaae and Sønderby, Søren Kaae and Winther, Ole},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1445--1453},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/maaloe16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/maaloe16.html},
  abstract = 	 {Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.}
}

@misc{bowmann-2016,
      title={Generating Sentences from a Continuous Space}, 
      author={Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},
      year={2016},
      eprint={1511.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{exp-family-jorden-2009,
  author        = {Michael Jordan },
  title         = {The Exponential Family: Basics},
  year          = {2009},
  publisher     ={University of California, Berkeley},
  url           = {https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/readings.html},
  pdf           = {https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf}
}

@misc{conjugates-jorden-2009,
  author        = {Michael Jordan },
  title         = {The Exponential Family: Conjugates},
  year          = {2009},
  publisher     ={University of California, Berkeley},
  url           = {https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/readings.html},
  pdf           = {https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf}
}