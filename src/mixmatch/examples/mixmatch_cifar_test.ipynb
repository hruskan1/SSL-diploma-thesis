{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import kornia as K\n",
    "import numpy as np\n",
    "import easydict\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import mixmatch\n",
    "import datasets\n",
    "import transformations as custom_transforms\n",
    "import utils\n",
    "import losses\n",
    "import models\n",
    "import ramps \n",
    "import wide_resnet\n",
    "\n",
    "# CIFAR 10 reference setting \n",
    "args = easydict.EasyDict()\n",
    "args.train_iterations = 100000\n",
    "args.K = 2\n",
    "args.T = 0.5\n",
    "args.alpha = 0.75\n",
    "args.lam_u = 75 \n",
    "args.rampup_length = 100000\n",
    "args.n_labeled = 250\n",
    "args.batch_size = 64\n",
    "args.lr = 0.002 \n",
    "args.ewa_coef = 0.95\n",
    "args.device = utils.get_device(1)\n",
    "args.cifar_root = './CIFAR10'\n",
    "args.cifar_download = True\n",
    "args.mean_teacher_coef = None\n",
    "\n",
    "args.basename = 'foo'\n",
    "args.call_prefix = '-2'\n",
    "args.res_path = './'\n",
    "args.new_log = False\n",
    "\n",
    "args.log_period = 1000\n",
    "args.save_period = 10000\n",
    "args.validation_period = 1000\n",
    "\n",
    "args.logpath = 'logs/log-' + args.basename + args.call_prefix + '.txt'\n",
    "args.logpath = os.path.join(args.res_path, args.logpath)\n",
    "\n",
    "args.model_path = 'models/' + args.basename + args.call_prefix +'-'\n",
    "args.model_path = os.path.join(args.res_path, args.model_path)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(args.logpath) or args.new_log:\n",
    "    print(f\"# Starting at {datetime.now()}\",file=open(args.logpath,'w'),flush=True)\n",
    "else:\n",
    "    print(f\"# Starting at {datetime.now()}\",file=open(args.logpath,'a'),flush=True)\n",
    "\n",
    "print(f\"with args:\\n\" + \"\\n\".join([f\"{key} : {value}\" for key,value in args.items()]),file=open(args.logpath,'a'),flush=True)\n",
    "print(f\"logpath: {args.logpath}\",file=open(args.logpath,'a'),flush=True)\n",
    "print(f\"modelpath: {args.model_path}<name>.pt\",file=open(args.logpath,'a'),flush=True)\n",
    " \n",
    "# Datasets and dataloaders\n",
    "\n",
    "labeled_dataloader, unlabeled_dataloader, validation_dataloader = datasets.get_CIFAR10(args.cifar_root,args.n_labeled,args.batch_size,download=args.cifar_download)\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 1.47M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# SGD\u001b[39;00m\n\u001b[1;32m    145\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 146\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    147\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39muse_ema: \n",
      "File \u001b[0;32m/datagrid/personal/hruskan1/miniconda3/envs/gpu/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/datagrid/personal/hruskan1/miniconda3/envs/gpu/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transforamtion\n",
    "#k1 = custom_transforms.GaussianNoiseChannelwise((0.0001, 0.0001, 0.0001))\n",
    "#k2 = K.augmentation.RandomGaussianBlur((3,3),sigma=(1.,1.),p = 0.5)\n",
    "k3 = K.augmentation.RandomHorizontalFlip(p=0.5)\n",
    "k4 = K.augmentation.RandomVerticalFlip(p=0.5)\n",
    "#k5 = K.augmentation.RandomAffine([-5., 5.], [0.1, 0.1], [0.8, 1.2], [0., 0.15])\n",
    "\n",
    "img_trans = nn.ModuleList([k3,k4])\n",
    "mask_trans = nn.ModuleList([k3,k4]) # only for segmentation \n",
    "invert_trans  = nn.ModuleList([k3,k4])\n",
    "num_classes = 10 \n",
    "transform = custom_transforms.MyAugmentation(img_trans,mask_trans,invert_trans)\n",
    "\n",
    "\n",
    "\n",
    "# Model, optimizer and eval_function\n",
    "model = wide_resnet.WideResNet(num_classes)\n",
    "opt = torch.optim.Adam(params=model.parameters(),lr = args.lr)\n",
    "eval_loss_fn = losses.kl_divergence\n",
    "\n",
    "# Load previous checkpoint\n",
    "if args.get('load_path',None) is not None:\n",
    "    print(f\"Loading checkpoint : {args.load_path}\",file=open(args.logpath, 'a'), flush=True)\n",
    "    count,metrics,net,opt,net_args = utils.load_checkpoint(args.device,model,opt,args.load_path) \n",
    "    ewa_loss = metrics['train_criterion_ewa'][-1]\n",
    "else:\n",
    "    print(\"Creating new network!\",file=open(args.logpath, 'a'), flush=True)\n",
    "    \n",
    "    metrics = easydict.EasyDict()\n",
    "    metrics['train_criterion'] = np.empty(0)\n",
    "    metrics['train_criterion_ewa'] = np.empty(0)\n",
    "    metrics['val_loss'] = np.empty(0)\n",
    "    metrics['val_acc'] = np.empty(0)\n",
    "    metrics['train_loss'] = np.empty(0)\n",
    "    metrics['train_acc'] = np.empty(0)\n",
    "    count = 0\n",
    "    ewa_loss = 0\n",
    "\n",
    "\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0),file=open(args.logpath, 'a'), flush=True)\n",
    "\n",
    "# Preparation for training function \n",
    "\n",
    "labeled_train_iter = iter(labeled_dataloader)\n",
    "unlabeled_train_iter = iter(unlabeled_dataloader)\n",
    "\n",
    "# Use Teacher if desired\n",
    "if args.mean_teacher_coef:\n",
    "        mixmatch_clf = models.Mean_Teacher(model,args.mean_teacher_coef)\n",
    "else:\n",
    "    mixmatch_clf = model\n",
    "\n",
    "model.train()\n",
    "model.to(args.device)\n",
    "\n",
    "# Iterate over index iterator until the desired number of iteration is achived\n",
    "while count < args.train_iterations:\n",
    "\n",
    "    if count == 0: # for first time\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,validation_dataloader,args.device)\n",
    "        metrics['val_loss'] = np.append(metrics['val_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['val_acc'] = np.append(metrics['val_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "        #\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,labeled_dataloader,args.device)\n",
    "        metrics['train_loss'] = np.append(metrics['train_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['train_acc'] = np.append(metrics['train_acc'],acc.detach().cpu().numpy())\n",
    "    \n",
    "    # Iterate over the end if necessary (Can be used with different sizes of dataloaders)\n",
    "    try:\n",
    "        data_l, labels = next(labeled_train_iter)\n",
    "    except:\n",
    "        labeled_train_iter = iter(labeled_dataloader)\n",
    "        data_l, labels = next(labeled_train_iter)\n",
    "\n",
    "\n",
    "    try:\n",
    "        data_u = next(unlabeled_train_iter)\n",
    "    except:\n",
    "        unlabeled_train_iter = iter(unlabeled_dataloader)\n",
    "        data_u = next(unlabeled_train_iter)\n",
    "\n",
    "    data_l = data_l.to(args.device)\n",
    "    labels = labels.to(args.device)\n",
    "    data_u = data_u.to(args.device)\n",
    "\n",
    "    # Corner case (batches with different sizes, namely for iregular last batch)\n",
    "    critical_count = None\n",
    "    \n",
    "    current_batch_size = min(data_l.shape[0],data_u.shape[0])\n",
    "    \n",
    "    data_l = data_l[:current_batch_size]\n",
    "    labels = labels[:current_batch_size]\n",
    "    data_u = data_u[:current_batch_size]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        l_batch,u_batch = mixmatch.mixmatch(labeled_batch=data_l,\n",
    "                                                labels=labels,\n",
    "                                                unlabeled_batch=data_u,\n",
    "                                                clf=mixmatch_clf,\n",
    "                                                augumentation=transform,\n",
    "                                                K=args.K,\n",
    "                                                T=args.T,\n",
    "                                                alpha=args.alpha\n",
    "                                                )\n",
    "        \n",
    "    x = torch.cat([l_batch[0],u_batch[0]],dim=0)\n",
    "    targets_l,targets_u = l_batch[1],u_batch[1]\n",
    "\n",
    "    # Interleave labeled and unlabeled samples between batches to obtain correct batchnorm calculation\n",
    "    x_splitted = list(torch.split(x, current_batch_size))\n",
    "    x_splitted = mixmatch.interleave(x_splitted, current_batch_size)\n",
    "    \n",
    "    # Forward \n",
    "    model.train() \n",
    "    logits = [model(x_splitted[0])]\n",
    "    for x in x_splitted[1:]:\n",
    "        logits.append(model(x))\n",
    "\n",
    "    # put interleaved samples back\n",
    "    logits = mixmatch.interleave(logits, current_batch_size)\n",
    "    logits_l = logits[0]\n",
    "    logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "    # Loss \n",
    "    # TODO: Deal with mask of valid regions of transformed images (fo affine transformation) -> remove black parts\n",
    "    loss_supervised = losses.soft_cross_entropy(logits_l,targets_l,reduction='mean')\n",
    "    loss_unsupervised = losses.mse_softmax(logits_u,targets_u,reduction='mean')\n",
    "\n",
    "    # Lx = -torch.mean(torch.sum(F.log_softmax(logits_l, dim=1) * targets_l, dim=1))\n",
    "    # Lu = torch.mean((torch.softmax(logits_u, dim=1) - targets_u)**2)\n",
    "    # print(f\"{loss_supervised=:.2f},{Lx=:.2f}\")\n",
    "    # print(f\"{loss_unsupervised=:.2f}{Lu=:.2f}\")\n",
    "\n",
    "    lam_u = ramps.linear_rampup(current = count, rampup_length = args.rampup_length) * args.lam_u\n",
    "    loss = loss_supervised + lam_u * loss_unsupervised\n",
    "\n",
    "    # SGD\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if args.mean_teacher_coef: \n",
    "        mixmatch_clf.update_weights(model)\n",
    "\n",
    "    # Ewa loss\n",
    "    if (count == 0 and ewa_loss == 0):\n",
    "        ewa_loss = loss        \n",
    "    else:\n",
    "        ewa_loss = args.ewa_coef * ewa_loss + (1-args.ewa_coef) * loss\n",
    "    \n",
    "    # Save loss (every time):\n",
    "    metrics['train_criterion'] = np.append(metrics['train_criterion'],loss.detach().cpu().numpy())\n",
    "    metrics['train_criterrion_ewa'] = np.append(metrics['train_criterion'],ewa_loss.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    # Compute validation metrics if validation period \n",
    "    if (count % args.validation_period == args.validation_period-1) or (count == args.train_iterations-1): # for first time\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,validation_dataloader,args.device)\n",
    "        metrics['val_loss'] = np.append(metrics['val_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['val_acc'] = np.append(metrics['val_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "        #\n",
    "        ls,acc = wide_resnet.evaluate(model,eval_loss_fn,labeled_dataloader,args.device)\n",
    "        metrics['train_loss'] = np.append(metrics['train_loss'],ls.detach().cpu().numpy())\n",
    "        metrics['train_acc'] = np.append(metrics['train_acc'],acc.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    # Print log if log period\n",
    "    if (count % args.log_period == args.log_period-1) or (count == args.train_iterations-1):\n",
    "        strtoprint = f\"batch iteration: {str(count)} \"+  \\\n",
    "                     f\"ewa loss: {ewa_loss:.2f} \" + \\\n",
    "                     f\"val loss: {metrics['val_loss'][-1]:.2f} \" + \\\n",
    "                     f\"val acc: {metrics['val_acc'][-1]:.2f} \" + \\\n",
    "                     f\"train loss: {metrics['train_loss'][-1]:.2f} \" + \\\n",
    "                     f\"train acc:  {metrics['train_acc'][-1]:.2f} \"\n",
    "        print(strtoprint, file=open(args.logpath, 'a'), flush=True)\n",
    "\n",
    "    # Save checkpoint if save_period\n",
    "    if (count % args.save_period == args.save_period-1) or (count == args.train_iterations-1):\n",
    "        m_path = args.model_path + f\"e{count}\" + '-m.pt'\n",
    "        print(f'# Saving Model : {m_path}', file=open(args.logpath, 'a'), flush=True)\n",
    "        utils.save_checkpoint(count,metrics,model,opt,args,m_path)\n",
    "    \n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
